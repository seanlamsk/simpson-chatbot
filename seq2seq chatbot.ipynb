{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq chatbot.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6N-FHAgYyYIa"},"source":["# Pytorch Seq2Seq Implementation\n","\n","Adapted from Matthew Inkawhich's Cornell Dialogue Chat bot tutorial https://github.com/MatthewInkawhich/pytorch-chatbot/blob/master/Chatbot_tutorial.ipynb\n","\n","Dataset retrieved from: https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons"]},{"cell_type":"code","metadata":{"id":"fNRr52PsO917","executionInfo":{"status":"ok","timestamp":1637654339884,"user_tz":-480,"elapsed":15700,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import unicode_literals\n","\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZaZ5gLn-sdm"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"omyGMmNB-6DV"},"source":["Our next order of business is to create a vocabulary and load\n","query/response sentence pairs into memory.\n","\n","Note that we are dealing with sequences of **words**, which do not have\n","an implicit mapping to a discrete numerical space. Thus, we must create\n","one by mapping each unique word that we encounter in our dataset to an\n","index value.\n","\n","For this we define a ``Voc`` class, which keeps a mapping from words to\n","indexes, a reverse mapping of indexes to words, a count of each word and\n","a total word count. The class provides methods for adding a word to the\n","vocabulary (``addWord``), adding all words in a sentence\n","(``addSentence``) and trimming infrequently seen words (``trim``). More\n","on trimming later."]},{"cell_type":"code","metadata":{"id":"D00uyP13-uM6","executionInfo":{"status":"ok","timestamp":1637654343307,"user_tz":-480,"elapsed":1685,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["# Default word tokens\n","PAD_token = 0  # Used for padding short sentences\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","\n","class Voc:\n","    def __init__(self):\n","        self.trimmed = False\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # Count SOS, EOS, PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.num_words\n","            self.word2count[word] = 1\n","            self.index2word[self.num_words] = word\n","            self.num_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    # Remove words below a certain count threshold\n","    def trim(self, min_count):\n","        if self.trimmed:\n","            return\n","        self.trimmed = True\n","\n","        keep_words = []\n","\n","        for k, v in self.word2count.items():\n","            if v >= min_count:\n","                keep_words.append(k)\n","\n","        print('keep_words {} / {} = {:.4f}'.format(\n","            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","        ))\n","\n","        # Reinitialize dictionaries\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3 # Count default tokens\n","\n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pl0b8l1N-1HT"},"source":["Now we can assemble our vocabulary and query/response sentence pairs.\n","Before we are ready to use this data, we must perform some\n","preprocessing.\n","\n","First, we must convert the Unicode strings to ASCII using\n","``unicodeToAscii``. Next, we should convert all letters to lowercase and\n","trim all non-letter characters except for basic punctuation\n","(``normalizeString``). Finally, to aid in training convergence, we will\n","filter out sentences with length greater than the ``MAX_LENGTH``\n","threshold (``filterPairs``)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5n5a8XtZ-8O-","executionInfo":{"status":"ok","timestamp":1637654358037,"user_tz":-480,"elapsed":14732,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"e0b19eb9-0b9f-4b60-d0a4-e8df85b8ed71"},"source":["DELIM = \"++++@++++\"\n","MAX_LENGTH = 10  # Maximum sentence length to consider\n","\n","# Turn a Unicode string to plain ASCII, thanks to\n","# http://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","# Read query/response pairs and return a voc object\n","def readVocs(datafile):\n","    print(\"Reading lines...\")\n","    # Read the file and split into lines\n","    lines = open(datafile, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split(DELIM)] for l in lines]\n","    voc = Voc()\n","    return voc, pairs\n","\n","# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","def filterPair(p):\n","    # Input sequences need to preserve the last word for EOS token\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","# Filter pairs using filterPair condition\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","# Using the functions defined above, return a populated voc object and pairs list\n","def loadPrepareData(datafile, save_dir):\n","    print(\"Start preparing training data ...\")\n","    voc, pairs = readVocs(datafile)\n","    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","    print(\"Counted words:\", voc.num_words)\n","    return voc, pairs\n","\n","\n","# Load/Assemble voc and pairs\n","save_dir = \"save/.txt\"\n","\n","datafile = \"qa_pairs.txt\"\n","\n","voc, pairs = loadPrepareData(datafile, save_dir)\n","# Print some pairs to validate\n","print(\"\\npairs:\")\n","for pair in pairs[:10]:\n","    print(pair)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Start preparing training data ...\n","Reading lines...\n","Read 18005 sentence pairs\n","Trimmed to 4087 sentence pairs\n","Counting words...\n","Counted words: 5071\n","\n","pairs:\n","['mr . bergstrom left today .', 'oh .']\n","['he s gone . forever .', 'and ?']\n","['no !', 'nuts .']\n","['that was our wedding .', 'oh .']\n","['you re so silly .', 'gimme a banana .']\n","['i don t have any banana .', 'come on you re holding out on me .']\n","['no .', 'would you have to do extra work ?']\n","['hey ! thanks monkey man .', 'holy moly . . . talk about parenting !']\n","['do we have enough glasses ?', 'do we have enough gag ice cubs ?']\n","['homer ! homer ! put a record on !', 'what are all our friends names again ?']\n"]}]},{"cell_type":"markdown","metadata":{"id":"iL13bp9TOJB7"},"source":["Another tactic that is beneficial to achieving faster convergence during\n","training is trimming rarely used words out of our vocabulary. Decreasing\n","the feature space will also soften the difficulty of the function that\n","the model must learn to approximate. We will do this as a two-step\n","process:\n","\n","1) Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim``\n","   function.\n","\n","2) Filter out pairs with trimmed words."]},{"cell_type":"code","metadata":{"id":"s8Z2JnxHOIcX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637654360257,"user_tz":-480,"elapsed":891,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"31f6b359-82b1-4ede-ad1e-5e8366969491"},"source":["MIN_COUNT = 3    # Minimum word count threshold for trimming\n","\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","    # Trim words used under the MIN_COUNT from the voc\n","    voc.trim(MIN_COUNT)\n","    # Filter out pairs with trimmed words\n","    keep_pairs = []\n","    for pair in pairs:\n","        input_sentence = pair[0]\n","        output_sentence = pair[1]\n","        keep_input = True\n","        keep_output = True\n","        # Check input sentence\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_input = False\n","                break\n","        # Check output sentence\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_output = False\n","                break\n","\n","        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","    return keep_pairs\n","\n","\n","# Trim voc and pairs\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["keep_words 1322 / 5068 = 0.2609\n","Trimmed from 4087 pairs to 1550, 0.3793 of total\n"]}]},{"cell_type":"markdown","metadata":{"id":"4LuUYxCbOOjB"},"source":["## Preparing Dataset"]},{"cell_type":"markdown","metadata":{"id":"-GGpm65fORs8"},"source":["Although we have spent a great effort preparing and massaging our data\n","into a nice vocabulary object and list of sentence pairs, our models\n","will ultimately expect numerical torch tensors as inputs. One way to\n","prepare the processed data for the models can be found in the `seq2seq\n","translation\n","tutorial <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__.\n","In that tutorial, we use a batch size of 1, meaning that all we have to\n","do is convert the words in our sentence pairs to their corresponding\n","indexes from the vocabulary and feed this to the models.\n","\n","However, if you’re interested in speeding up training and/or would like\n","to leverage GPU parallelization capabilities, you will need to train\n","with mini-batches.\n","\n","Using mini-batches also means that we must be mindful of the variation\n","of sentence length in our batches. To accomodate sentences of different\n","sizes in the same batch, we will make our batched input tensor of shape\n","*(max_length, batch_size)*, where sentences shorter than the\n","*max_length* are zero padded after an *EOS_token*.\n","\n","If we simply convert our English sentences to tensors by converting\n","words to their indexes(\\ ``indexesFromSentence``) and zero-pad, our\n","tensor would have shape *(batch_size, max_length)* and indexing the\n","first dimension would return a full sequence across all time-steps.\n","However, we need to be able to index our batch along time, and across\n","all sequences in the batch. Therefore, we transpose our input batch\n","shape to *(max_length, batch_size)*, so that indexing across the first\n","dimension returns a time step across all sentences in the batch. We\n","handle this transpose implicitly in the ``zeroPadding`` function.\n","\n","![RNN Bidirectional 2](https://github.com/MatthewInkawhich/pytorch-chatbot/blob/master/images/seq2seq_batches.png?raw=true) \n","\n","The ``inputVar`` function handles the process of converting sentences to\n","tensor, ultimately creating a correctly shaped zero-padded tensor. It\n","also returns a tensor of ``lengths`` for each of the sequences in the\n","batch which will be passed to our decoder later.\n","\n","The ``outputVar`` function performs a similar function to ``inputVar``,\n","but instead of returning a ``lengths`` tensor, it returns a binary mask\n","tensor and a maximum target sentence length. The binary mask tensor has\n","the same shape as the output target tensor, but every element that is a\n","*PAD_token* is 0 and all others are 1.\n","\n","``batch2TrainData`` simply takes a bunch of pairs and returns the input\n","and target tensors using the aforementioned functions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGmSY3WCOVq2","executionInfo":{"status":"ok","timestamp":1637654361763,"user_tz":-480,"elapsed":465,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"adfe3cd7-12d5-419c-c5bb-1c3d13b8dfab"},"source":["def indexesFromSentence(voc, sentence):\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n","\n","\n","def zeroPadding(l, fillvalue=PAD_token):\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","def binaryMatrix(l, value=PAD_token):\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_token:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","    return m\n","\n","# Returns padded input sequence tensor and lengths\n","def inputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","\n","# Returns padded target sequence tensor, padding mask, and max target length\n","def outputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    mask = binaryMatrix(padList)\n","    mask = torch.ByteTensor(mask)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, mask, max_target_len\n","\n","# Returns all items for a given batch of pairs\n","def batch2TrainData(voc, pair_batch):\n","    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n","    input_batch, output_batch = [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","    inp, lengths = inputVar(input_batch, voc)\n","    output, mask, max_target_len = outputVar(output_batch, voc)\n","    return inp, lengths, output, mask, max_target_len\n","\n","\n","# Example for validation\n","small_batch_size = 5\n","batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n","input_variable, lengths, target_variable, mask, max_target_len = batches\n","\n","print(\"input_variable:\", input_variable)\n","print(\"lengths:\", lengths)\n","print(\"target_variable:\", target_variable)\n","print(\"mask:\", mask)\n","print(\"max_target_len:\", max_target_len)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["input_variable: tensor([[ 219,   54,  201,  574, 1092],\n","        [ 116,  620,  104,  766,  688],\n","        [   4,   90,  116,    4,    4],\n","        [   4,  615,   15,    2,    2],\n","        [   4, 1217,    2,    0,    0],\n","        [  84,    4,    0,    0,    0],\n","        [1172,    2,    0,    0,    0],\n","        [   4,    0,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]])\n","lengths: tensor([9, 7, 5, 4, 4])\n","target_variable: tensor([[  89,   28,  226,  460,   56],\n","        [  39,   29,  260,   15,   84],\n","        [  28,   30,   84,    2,   13],\n","        [  78,   31,  210,    0,    2],\n","        [ 341,  113,  241,    0,    0],\n","        [  67,  615,  962,    0,    0],\n","        [  13, 1217,    4,    0,    0],\n","        [   2,   15,    2,    0,    0],\n","        [   0,    2,    0,    0,    0]])\n","mask: tensor([[1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 1, 1, 0, 1],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n","max_target_len: 9\n"]}]},{"cell_type":"markdown","metadata":{"id":"IWR_qIiZOXhr"},"source":["## Define Models"]},{"cell_type":"markdown","metadata":{"id":"XnjVAY-2ObIT"},"source":["Seq2Seq Model\n","\n","The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The\n","goal of a seq2seq model is to take a variable-length sequence as an\n","input, and return a variable-length sequence as an output using a\n","fixed-sized model.\n","\n","`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that\n","by using two separate recurrent neural nets together, we can accomplish\n","this task. One RNN acts as an **encoder**, which encodes a variable\n","length input sequence to a fixed-length context vector. In theory, this\n","context vector (the final hidden layer of the RNN) will contain semantic\n","information about the query sentence that is input to the bot. The\n","second RNN is a **decoder**, which takes an input word and the context\n","vector, and returns a guess for the next word in the sequence and a\n","hidden state to use in the next iteration.\n","\n","![seq2seq_ts](https://github.com/MatthewInkawhich/pytorch-chatbot/blob/master/images/seq2seq_ts.png?raw=true) \n","\n","\n","Image source:\n","https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/\n"]},{"cell_type":"markdown","metadata":{"id":"_HIJu71QOeM_"},"source":["### Encoder"]},{"cell_type":"markdown","metadata":{"id":"x85d5YhgOi4K"},"source":["![RNN Bidirectional 2](https://github.com/MatthewInkawhich/pytorch-chatbot/blob/master/images/RNN-bidirectional.png?raw=true) \n","\n","Image source: http://colah.github.io/posts/2015-09-NN-Types-FP/\n","\n","**Computation Graph:**\n","~~~\n","   1) Convert word indexes to embeddings.\n","   2) Pack padded batch of sequences for RNN module.\n","   3) Forward pass through GRU.\n","   4) Unpack padding.\n","   5) Sum bidirectional GRU outputs.\n","   6) Return output and final hidden state.\n","~~~\n","**Inputs:**\n","\n","-  ``input_seq``: batch of input sentences; shape=\\ *(max_length,\n","   batch_size)*\n","-  ``input_lengths``: list of sentence lengths corresponding to each\n","   sentence in the batch; shape=\\ *(batch_size)*\n","-  ``hidden``: hidden state; shape=\\ *(n_layers x num_directions,\n","   batch_size, hidden_size)*\n","\n","**Outputs:**\n","\n","-  ``outputs``: output features from the last hidden layer of the GRU\n","   (sum of bidirectional outputs); shape=\\ *(max_length, batch_size,\n","   hidden_size)*\n","-  ``hidden``: updated hidden state from GRU; shape=\\ *(n_layers x\n","   num_directions, batch_size, hidden_size)*"]},{"cell_type":"code","metadata":{"id":"zA07TuKZObtG","executionInfo":{"status":"ok","timestamp":1637654377211,"user_tz":-480,"elapsed":284,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n","        super(EncoderRNN, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embedding = embedding\n","\n","        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n","        #   because our input size is a word embedding with number of features == hidden_size\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","\n","    def forward(self, input_seq, input_lengths, hidden=None):\n","        # Convert word indexes to embeddings\n","        embedded = self.embedding(input_seq)\n","        # Pack padded batch of sequences for RNN module\n","        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n","        # Forward pass through GRU\n","        outputs, hidden = self.gru(packed, hidden)\n","        # Unpack padding\n","        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n","        # Sum bidirectional GRU outputs\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        # Return output and final hidden state\n","        return outputs, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7K8ch71PDwR"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"Xe76TDYsPNQo"},"source":["#### Attention Module\n","https://ai.plainenglish.io/introduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da"]},{"cell_type":"code","metadata":{"id":"5__8r2mYPMhC","executionInfo":{"status":"ok","timestamp":1637654380932,"user_tz":-480,"elapsed":577,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["# Attention Layer\n","class Attn(torch.nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method\n","        if self.method not in ['dot', 'general', 'concat']: # Luong, self, Bahdanau\n","            raise ValueError(self.method, \"is not an appropriate attention method.\")\n","        self.hidden_size = hidden_size\n","        if self.method == 'general':\n","            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n","        elif self.method == 'concat':\n","            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n","            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n","\n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)\n","\n","    def concat_score(self, hidden, encoder_output):\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n","        return torch.sum(self.v * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # Calculate the attention weights (energies) based on the given method\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'concat':\n","            attn_energies = self.concat_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        # Transpose max_length and batch_size dimensions\n","        attn_energies = attn_energies.t()\n","\n","        # Return the softmax normalized probability scores (with added dimension)\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJ9ol8dWPR6u"},"source":["**Computation Graph:**\n","\n","   1) Get embedding of current input word.\n","   2) Forward through unidirectional GRU.\n","   3) Calculate attention weights from the current GRU output from (2).\n","   4) Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector.\n","   5) Concatenate weighted context vector and GRU output using Luong eq. 5.\n","   6) Predict next word using Luong eq. 6 (without softmax).\n","   7) Return output and final hidden state.\n","\n","**Inputs:**\n","\n","-  ``input_step``: one time step (one word) of input sequence batch;\n","   shape=\\ *(1, batch_size)*\n","-  ``last_hidden``: final hidden layer of GRU; shape=\\ *(n_layers x\n","   num_directions, batch_size, hidden_size)*\n","-  ``encoder_outputs``: encoder model’s output; shape=\\ *(max_length,\n","   batch_size, hidden_size)*\n","\n","**Outputs:**\n","\n","-  ``output``: softmax normalized tensor giving probabilities of each\n","   word being the correct next word in the decoded sequence;\n","   shape=\\ *(batch_size, voc.num_words)*\n","-  ``hidden``: final hidden state of GRU; shape=\\ *(n_layers x\n","   num_directions, batch_size, hidden_size)*"]},{"cell_type":"markdown","metadata":{"id":"-dH3vcXYPaK8"},"source":["#### Decode Model"]},{"cell_type":"code","metadata":{"id":"q2jIp5rqPXFH","executionInfo":{"status":"ok","timestamp":1637654384575,"user_tz":-480,"elapsed":285,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["class LuongAttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(LuongAttnDecoderRNN, self).__init__()\n","\n","        # Keep for reference\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # Define layers\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # Note: we run this one step (word) at a time\n","        # Get embedding of current input word\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # Forward through unidirectional GRU\n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","        # Calculate attention weights from the current GRU output\n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Concatenate weighted context vector and GRU output using Luong eq. 5\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Predict next word using Luong eq. 6\n","        output = self.out(concat_output)\n","        output = F.softmax(output, dim=1)\n","        # Return output and final hidden state\n","        return output, hidden"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJySlb54epqN","executionInfo":{"status":"ok","timestamp":1637654391495,"user_tz":-480,"elapsed":329,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(DecoderRNN, self).__init__()\n","\n","        # Keep for reference\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # Define layers\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # Note: we run this one step (word) at a time\n","        # Get embedding of current input word\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # Forward through unidirectional GRU\n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","        # Calculate attention weights from the current GRU output\n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Concatenate weighted context vector and GRU output using Luong eq. 5\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Predict next word using Luong eq. 6\n","        output = self.out(concat_output)\n","        output = F.softmax(output, dim=1)\n","        # Return output and final hidden state\n","        return output, hidden"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sutAEJ5QPgTj"},"source":["## Define Training Procedure"]},{"cell_type":"markdown","metadata":{"id":"eQdcGOk3Pm_-"},"source":["### Masked Loss"]},{"cell_type":"markdown","metadata":{"id":"bE-QamiRPp8h"},"source":["Masked loss\n","~~~~~~~~~~~\n","\n","Since we are dealing with batches of padded sequences, we cannot simply\n","consider all elements of the tensor when calculating loss. We define\n","``maskNLLLoss`` to calculate our loss based on our decoder’s output\n","tensor, the target tensor, and a binary mask tensor describing the\n","padding of the target tensor. This loss function calculates the average\n","negative log likelihood of the elements that correspond to a *1* in the\n","mask tensor."]},{"cell_type":"code","metadata":{"id":"8WYddLXlPoms","executionInfo":{"status":"ok","timestamp":1637654396939,"user_tz":-480,"elapsed":267,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["def maskNLLLoss(inp, target, mask):\n","    nTotal = mask.sum()\n","    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n","    loss = crossEntropy.masked_select(mask).mean()\n","    loss = loss.to(device)\n","    return loss, nTotal.item()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeQT9jtMP2-u"},"source":["### Single Training Iteration"]},{"cell_type":"markdown","metadata":{"id":"0FCxiDJeP1I5"},"source":["The ``train`` function contains the algorithm for a single training\n","iteration (a single batch of inputs).\n","\n","We will use a couple of clever tricks to aid in convergence:\n","\n","-  The first trick is using **teacher forcing**. This means that at some\n","   probability, set by ``teacher_forcing_ratio``, we use the current\n","   target word as the decoder’s next input rather than using the\n","   decoder’s current guess. This technique acts as training wheels for\n","   the decoder, aiding in more efficient training. However, teacher\n","   forcing can lead to model instability during inference, as the\n","   decoder may not have a sufficient chance to truly craft its own\n","   output sequences during training. Thus, we must be mindful of how we\n","   are setting the ``teacher_forcing_ratio``, and not be fooled by fast\n","   convergence.\n","\n","-  The second trick that we implement is **gradient clipping**. This is\n","   a commonly used technique for countering the “exploding gradient”\n","   problem. In essence, by clipping or thresholding gradients to a\n","   maximum value, we prevent the gradients from growing exponentially\n","   and either overflow (NaN), or overshoot steep cliffs in the cost\n","   function.\n","\n","![RNN Bidirectional 2](https://github.com/MatthewInkawhich/pytorch-chatbot/blob/master/images/grad_clip.png?raw=true) \n","\n","Image source: Goodfellow et al. *Deep Learning*. 2016. http://www.deeplearningbook.org/\n","\n","**Sequence of Operations:**\n","\n","   1) Forward pass entire input batch through encoder.\n","\n","   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n","\n","   3) Forward input batch sequence through decoder one time step at a time.\n","\n","   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n","\n","   5) Calculate and accumulate loss.\n","\n","   6) Perform backpropagation.\n","\n","   7) Clip gradients.\n","   \n","   8) Update encoder and decoder model parameters.\n","\n","\n",".. Note ::\n","\n","  PyTorch’s RNN modules (``RNN``, ``LSTM``, ``GRU``) can be used like any\n","  other non-recurrent layers by simply passing them the entire input\n","  sequence (or batch of sequences). We use the ``GRU`` layer like this in\n","  the ``encoder``. The reality is that under the hood, there is an\n","  iterative process looping over each time step calculating hidden states.\n","  Alternatively, you ran run these modules one time-step at a time. In\n","  this case, we manually loop over the sequences during the training\n","  process like we must do for the ``decoder`` model. As long as you\n","  maintain the correct conceptual model of these modules, implementing\n","  sequential models can be very straightforward.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"inhbcOXnP-tU","executionInfo":{"status":"ok","timestamp":1637654614475,"user_tz":-480,"elapsed":1285,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n","          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n","\n","    # Zero gradients\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Set device options\n","    input_variable = input_variable.to(device)\n","    lengths = lengths\n","    target_variable = target_variable.to(device)\n","    mask = mask.to(device)\n","\n","    # Initialize variables\n","    loss = 0\n","    print_losses = []\n","    n_totals = 0\n","\n","    # Forward pass through encoder\n","    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n","\n","    # Create initial decoder input (start with SOS tokens for each sentence)\n","    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n","    decoder_input = decoder_input.to(device)\n","\n","    # Set initial decoder hidden state to the encoder's final hidden state\n","    decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","    # Determine if we are using teacher forcing this iteration\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Forward batch of sequences through decoder one time step at a time\n","    if use_teacher_forcing:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            # Teacher forcing: next input is current target\n","            decoder_input = target_variable[t].view(1, -1)\n","            # Calculate and accumulate loss\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","    else:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            # No teacher forcing: next input is decoder's own current output\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","            decoder_input = decoder_input.to(device)\n","            # Calculate and accumulate loss\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","\n","    # Perform backpropatation\n","    loss.backward()\n","\n","    # Clip gradients: gradients are modified in place\n","    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n","    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n","\n","    # Adjust model weights\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return sum(print_losses) / n_totals"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X8tf3a9fQnjW"},"source":["### Training Loop"]},{"cell_type":"code","metadata":{"id":"TGfzPVyiQr_s","executionInfo":{"status":"ok","timestamp":1637654406553,"user_tz":-480,"elapsed":1111,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n","\n","    # Load batches for each iteration\n","    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n","                      for _ in range(n_iteration)]\n","\n","    # Initializations\n","    print('Initializing ...')\n","    start_iteration = 1\n","    print_loss = 0\n","    if loadFilename:\n","        start_iteration = checkpoint['iteration'] + 1\n","\n","    # Training loop\n","    print(\"Training...\")\n","    for iteration in range(start_iteration, n_iteration + 1):\n","        training_batch = training_batches[iteration - 1]\n","        # Extract fields from batch\n","        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n","\n","        # Run a training iteration with batch\n","        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n","                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n","        print_loss += loss\n","\n","        # Print progress\n","        if iteration % print_every == 0:\n","            print_loss_avg = print_loss / print_every\n","            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n","            print_loss = 0\n","\n","        # Save checkpoint\n","        if (iteration % save_every == 0):\n","            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            torch.save({\n","                'iteration': iteration,\n","                'en': encoder.state_dict(),\n","                'de': decoder.state_dict(),\n","                'en_opt': encoder_optimizer.state_dict(),\n","                'de_opt': decoder_optimizer.state_dict(),\n","                'loss': loss,\n","                'voc_dict': voc.__dict__,\n","                'embedding': embedding.state_dict()\n","            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mgX5zXo5QtoI"},"source":["## Define Evaluation"]},{"cell_type":"code","metadata":{"id":"DSgE-QEaQvv-","executionInfo":{"status":"ok","timestamp":1637654410488,"user_tz":-480,"elapsed":1297,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["class GreedySearchDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(GreedySearchDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, input_length, max_length):\n","        # Forward input through encoder model\n","        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n","        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n","        decoder_hidden = encoder_hidden[:decoder.n_layers]\n","        # Initialize decoder input with SOS_token\n","        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n","        # Initialize tensors to append decoded words to\n","        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n","        all_scores = torch.zeros([0], device=device)\n","        # Iteratively decode one word token at a time\n","        for _ in range(max_length):\n","            # Forward pass through decoder\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            # Obtain most likely word token and its softmax score\n","            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n","            # Record token and score\n","            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n","            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n","            # Prepare current token to be next decoder input (add a dimension)\n","            decoder_input = torch.unsqueeze(decoder_input, 0)\n","        # Return collections of word tokens and scores\n","        return all_tokens, all_scores"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sKxFaVJQ0Kf","executionInfo":{"status":"ok","timestamp":1637655135611,"user_tz":-480,"elapsed":852,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}}},"source":["def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n","    ### Format input sentence as a batch\n","    # words -> indexes\n","    indexes_batch = [indexesFromSentence(voc, sentence)]\n","    # Create lengths tensor\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    # Transpose dimensions of batch to match models' expectations\n","    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n","    # Use appropriate device\n","    input_batch = input_batch.to(device)\n","    lengths = lengths\n","    # Decode sentence with searcher\n","    tokens, scores = searcher(input_batch, lengths, max_length)\n","    # indexes -> words\n","    decoded_words = [voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","\n","\n","def evaluateInput(encoder, decoder, searcher, voc, print_output=True):\n","    input_sentence = ''\n","    while(1):\n","        try:\n","            # Get input sentence\n","            input_sentence = input('> ')\n","            # Check if it is quit case\n","            if input_sentence == 'q' or input_sentence == 'quit': break\n","            # Normalize sentence\n","            input_sentence = normalizeString(input_sentence)\n","            # Evaluate sentence\n","            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n","            # Format and print response sentence\n","            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","            if print_output:\n","                print(f'Homer Bot:', ' '.join(output_words))\n","\n","        except KeyError:\n","            print(\"Error: Encountered unknown word.\")"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Kd5-pxvQ45L"},"source":["## Run Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JCZEXNKQ-VN","executionInfo":{"status":"ok","timestamp":1637654455799,"user_tz":-480,"elapsed":40458,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"2fe84a34-25a2-48e6-d5d5-1a9e437e88d4"},"source":["# Configure models\n","model_name = 'cb_model'\n","corpus_name = 'simpson'\n","attn_model = 'dot'\n","#attn_model = 'general'\n","#attn_model = 'concat'\n","hidden_size = 500\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","batch_size = 64\n","\n","# Set checkpoint to load from; set to None if starting from scratch\n","loadFilename = None\n","checkpoint_iter = 4000\n","#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n","#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n","#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n","\n","\n","# Load model if a loadFilename is provided\n","if loadFilename:\n","    # If loading on same machine the model was trained on\n","    checkpoint = torch.load(loadFilename)\n","    # If loading a model trained on GPU to CPU\n","    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n","    encoder_sd = checkpoint['en']\n","    decoder_sd = checkpoint['de']\n","    encoder_optimizer_sd = checkpoint['en_opt']\n","    decoder_optimizer_sd = checkpoint['de_opt']\n","    embedding_sd = checkpoint['embedding']\n","    voc.__dict__ = checkpoint['voc_dict']\n","\n","\n","print('Building encoder and decoder ...')\n","# Initialize word embeddings\n","embedding = nn.Embedding(voc.num_words, hidden_size)\n","if loadFilename:\n","    embedding.load_state_dict(embedding_sd)\n","# Initialize encoder & decoder models\n","encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n","decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n","if loadFilename:\n","    encoder.load_state_dict(encoder_sd)\n","    decoder.load_state_dict(decoder_sd)\n","# Use appropriate device\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)\n","print('Models built and ready to go!')"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Building encoder and decoder ...\n","Models built and ready to go!\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z8dJ3APZRCqJ"},"source":["### Run training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BryQRp1RGDY","executionInfo":{"status":"ok","timestamp":1637655097470,"user_tz":-480,"elapsed":467383,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"82f9ba44-6e87-43a5-d37f-e3a58ef13ac5"},"source":["N_TEST = 20\n","test_pairs = pairs[:N_TEST]\n","train_pairs = pairs[N_TEST:]\n","\n","# Configure training/optimization\n","clip = 50.0\n","teacher_forcing_ratio = 1.0\n","learning_rate = 0.0001\n","decoder_learning_ratio = 5.0\n","n_iteration = 1000\n","print_every = 1\n","save_every = 500\n","\n","# Ensure dropout layers are in train mode\n","encoder.train()\n","decoder.train()\n","\n","# Initialize optimizers\n","print('Building optimizers ...')\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n","if loadFilename:\n","    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n","    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n","\n","# Run training iterations\n","print(\"Starting Training!\")\n","trainIters(model_name, voc, train_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n","           print_every, save_every, clip, corpus_name, loadFilename)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Building optimizers ...\n","Starting Training!\n","Initializing ...\n","Training...\n","Iteration: 1; Percent complete: 0.1%; Average loss: 3.3264\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  after removing the cwd from sys.path.\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 2; Percent complete: 0.2%; Average loss: 3.1809\n","Iteration: 3; Percent complete: 0.3%; Average loss: 3.0100\n","Iteration: 4; Percent complete: 0.4%; Average loss: 3.1375\n","Iteration: 5; Percent complete: 0.5%; Average loss: 3.2740\n","Iteration: 6; Percent complete: 0.6%; Average loss: 3.2541\n","Iteration: 7; Percent complete: 0.7%; Average loss: 3.3814\n","Iteration: 8; Percent complete: 0.8%; Average loss: 3.3915\n","Iteration: 9; Percent complete: 0.9%; Average loss: 3.4526\n","Iteration: 10; Percent complete: 1.0%; Average loss: 3.4503\n","Iteration: 11; Percent complete: 1.1%; Average loss: 3.3885\n","Iteration: 12; Percent complete: 1.2%; Average loss: 3.1525\n","Iteration: 13; Percent complete: 1.3%; Average loss: 3.3439\n","Iteration: 14; Percent complete: 1.4%; Average loss: 3.1046\n","Iteration: 15; Percent complete: 1.5%; Average loss: 3.0927\n","Iteration: 16; Percent complete: 1.6%; Average loss: 3.2762\n","Iteration: 17; Percent complete: 1.7%; Average loss: 3.2467\n","Iteration: 18; Percent complete: 1.8%; Average loss: 3.1469\n","Iteration: 19; Percent complete: 1.9%; Average loss: 3.2106\n","Iteration: 20; Percent complete: 2.0%; Average loss: 3.0411\n","Iteration: 21; Percent complete: 2.1%; Average loss: 3.4412\n","Iteration: 22; Percent complete: 2.2%; Average loss: 3.2343\n","Iteration: 23; Percent complete: 2.3%; Average loss: 3.2961\n","Iteration: 24; Percent complete: 2.4%; Average loss: 3.2056\n","Iteration: 25; Percent complete: 2.5%; Average loss: 3.0919\n","Iteration: 26; Percent complete: 2.6%; Average loss: 3.0966\n","Iteration: 27; Percent complete: 2.7%; Average loss: 3.0319\n","Iteration: 28; Percent complete: 2.8%; Average loss: 3.1186\n","Iteration: 29; Percent complete: 2.9%; Average loss: 2.9395\n","Iteration: 30; Percent complete: 3.0%; Average loss: 3.0907\n","Iteration: 31; Percent complete: 3.1%; Average loss: 3.1565\n","Iteration: 32; Percent complete: 3.2%; Average loss: 2.9469\n","Iteration: 33; Percent complete: 3.3%; Average loss: 2.9579\n","Iteration: 34; Percent complete: 3.4%; Average loss: 2.9182\n","Iteration: 35; Percent complete: 3.5%; Average loss: 2.9993\n","Iteration: 36; Percent complete: 3.6%; Average loss: 3.0538\n","Iteration: 37; Percent complete: 3.7%; Average loss: 2.8897\n","Iteration: 38; Percent complete: 3.8%; Average loss: 2.9585\n","Iteration: 39; Percent complete: 3.9%; Average loss: 3.1009\n","Iteration: 40; Percent complete: 4.0%; Average loss: 2.9830\n","Iteration: 41; Percent complete: 4.1%; Average loss: 3.0501\n","Iteration: 42; Percent complete: 4.2%; Average loss: 3.0408\n","Iteration: 43; Percent complete: 4.3%; Average loss: 2.9612\n","Iteration: 44; Percent complete: 4.4%; Average loss: 3.0716\n","Iteration: 45; Percent complete: 4.5%; Average loss: 2.9061\n","Iteration: 46; Percent complete: 4.6%; Average loss: 2.9111\n","Iteration: 47; Percent complete: 4.7%; Average loss: 2.9632\n","Iteration: 48; Percent complete: 4.8%; Average loss: 2.8610\n","Iteration: 49; Percent complete: 4.9%; Average loss: 3.0090\n","Iteration: 50; Percent complete: 5.0%; Average loss: 2.9015\n","Iteration: 51; Percent complete: 5.1%; Average loss: 2.8565\n","Iteration: 52; Percent complete: 5.2%; Average loss: 2.9642\n","Iteration: 53; Percent complete: 5.3%; Average loss: 3.0708\n","Iteration: 54; Percent complete: 5.4%; Average loss: 3.1335\n","Iteration: 55; Percent complete: 5.5%; Average loss: 2.5706\n","Iteration: 56; Percent complete: 5.6%; Average loss: 2.9970\n","Iteration: 57; Percent complete: 5.7%; Average loss: 2.7645\n","Iteration: 58; Percent complete: 5.8%; Average loss: 2.7850\n","Iteration: 59; Percent complete: 5.9%; Average loss: 2.7340\n","Iteration: 60; Percent complete: 6.0%; Average loss: 3.0807\n","Iteration: 61; Percent complete: 6.1%; Average loss: 2.8141\n","Iteration: 62; Percent complete: 6.2%; Average loss: 2.7423\n","Iteration: 63; Percent complete: 6.3%; Average loss: 2.6917\n","Iteration: 64; Percent complete: 6.4%; Average loss: 2.7685\n","Iteration: 65; Percent complete: 6.5%; Average loss: 2.6099\n","Iteration: 66; Percent complete: 6.6%; Average loss: 2.7367\n","Iteration: 67; Percent complete: 6.7%; Average loss: 2.6889\n","Iteration: 68; Percent complete: 6.8%; Average loss: 2.7538\n","Iteration: 69; Percent complete: 6.9%; Average loss: 2.6882\n","Iteration: 70; Percent complete: 7.0%; Average loss: 2.9074\n","Iteration: 71; Percent complete: 7.1%; Average loss: 2.9656\n","Iteration: 72; Percent complete: 7.2%; Average loss: 2.6121\n","Iteration: 73; Percent complete: 7.3%; Average loss: 2.8779\n","Iteration: 74; Percent complete: 7.4%; Average loss: 2.7778\n","Iteration: 75; Percent complete: 7.5%; Average loss: 2.7288\n","Iteration: 76; Percent complete: 7.6%; Average loss: 2.6148\n","Iteration: 77; Percent complete: 7.7%; Average loss: 2.6243\n","Iteration: 78; Percent complete: 7.8%; Average loss: 2.5288\n","Iteration: 79; Percent complete: 7.9%; Average loss: 2.9044\n","Iteration: 80; Percent complete: 8.0%; Average loss: 2.7356\n","Iteration: 81; Percent complete: 8.1%; Average loss: 2.6869\n","Iteration: 82; Percent complete: 8.2%; Average loss: 2.7785\n","Iteration: 83; Percent complete: 8.3%; Average loss: 2.7942\n","Iteration: 84; Percent complete: 8.4%; Average loss: 2.4465\n","Iteration: 85; Percent complete: 8.5%; Average loss: 2.5954\n","Iteration: 86; Percent complete: 8.6%; Average loss: 2.7916\n","Iteration: 87; Percent complete: 8.7%; Average loss: 2.8187\n","Iteration: 88; Percent complete: 8.8%; Average loss: 2.8594\n","Iteration: 89; Percent complete: 8.9%; Average loss: 2.4054\n","Iteration: 90; Percent complete: 9.0%; Average loss: 2.7555\n","Iteration: 91; Percent complete: 9.1%; Average loss: 2.7149\n","Iteration: 92; Percent complete: 9.2%; Average loss: 2.8220\n","Iteration: 93; Percent complete: 9.3%; Average loss: 2.3815\n","Iteration: 94; Percent complete: 9.4%; Average loss: 2.7826\n","Iteration: 95; Percent complete: 9.5%; Average loss: 2.5072\n","Iteration: 96; Percent complete: 9.6%; Average loss: 2.4653\n","Iteration: 97; Percent complete: 9.7%; Average loss: 2.5459\n","Iteration: 98; Percent complete: 9.8%; Average loss: 2.5666\n","Iteration: 99; Percent complete: 9.9%; Average loss: 2.7139\n","Iteration: 100; Percent complete: 10.0%; Average loss: 2.6262\n","Iteration: 101; Percent complete: 10.1%; Average loss: 2.2889\n","Iteration: 102; Percent complete: 10.2%; Average loss: 2.5184\n","Iteration: 103; Percent complete: 10.3%; Average loss: 2.7599\n","Iteration: 104; Percent complete: 10.4%; Average loss: 2.6221\n","Iteration: 105; Percent complete: 10.5%; Average loss: 2.5401\n","Iteration: 106; Percent complete: 10.6%; Average loss: 2.4455\n","Iteration: 107; Percent complete: 10.7%; Average loss: 2.5105\n","Iteration: 108; Percent complete: 10.8%; Average loss: 2.7932\n","Iteration: 109; Percent complete: 10.9%; Average loss: 2.6685\n","Iteration: 110; Percent complete: 11.0%; Average loss: 2.5255\n","Iteration: 111; Percent complete: 11.1%; Average loss: 2.4756\n","Iteration: 112; Percent complete: 11.2%; Average loss: 2.4533\n","Iteration: 113; Percent complete: 11.3%; Average loss: 2.3929\n","Iteration: 114; Percent complete: 11.4%; Average loss: 2.5645\n","Iteration: 115; Percent complete: 11.5%; Average loss: 2.5106\n","Iteration: 116; Percent complete: 11.6%; Average loss: 2.4671\n","Iteration: 117; Percent complete: 11.7%; Average loss: 2.4157\n","Iteration: 118; Percent complete: 11.8%; Average loss: 2.3698\n","Iteration: 119; Percent complete: 11.9%; Average loss: 2.5617\n","Iteration: 120; Percent complete: 12.0%; Average loss: 2.3689\n","Iteration: 121; Percent complete: 12.1%; Average loss: 2.4163\n","Iteration: 122; Percent complete: 12.2%; Average loss: 2.4094\n","Iteration: 123; Percent complete: 12.3%; Average loss: 2.4371\n","Iteration: 124; Percent complete: 12.4%; Average loss: 2.5165\n","Iteration: 125; Percent complete: 12.5%; Average loss: 2.4341\n","Iteration: 126; Percent complete: 12.6%; Average loss: 2.5351\n","Iteration: 127; Percent complete: 12.7%; Average loss: 2.4015\n","Iteration: 128; Percent complete: 12.8%; Average loss: 2.5029\n","Iteration: 129; Percent complete: 12.9%; Average loss: 2.5150\n","Iteration: 130; Percent complete: 13.0%; Average loss: 2.4144\n","Iteration: 131; Percent complete: 13.1%; Average loss: 2.4494\n","Iteration: 132; Percent complete: 13.2%; Average loss: 2.2883\n","Iteration: 133; Percent complete: 13.3%; Average loss: 2.2994\n","Iteration: 134; Percent complete: 13.4%; Average loss: 2.3196\n","Iteration: 135; Percent complete: 13.5%; Average loss: 2.0616\n","Iteration: 136; Percent complete: 13.6%; Average loss: 2.2255\n","Iteration: 137; Percent complete: 13.7%; Average loss: 2.4935\n","Iteration: 138; Percent complete: 13.8%; Average loss: 2.4195\n","Iteration: 139; Percent complete: 13.9%; Average loss: 2.3312\n","Iteration: 140; Percent complete: 14.0%; Average loss: 2.4250\n","Iteration: 141; Percent complete: 14.1%; Average loss: 2.4778\n","Iteration: 142; Percent complete: 14.2%; Average loss: 2.4204\n","Iteration: 143; Percent complete: 14.3%; Average loss: 2.2966\n","Iteration: 144; Percent complete: 14.4%; Average loss: 2.3181\n","Iteration: 145; Percent complete: 14.5%; Average loss: 2.3185\n","Iteration: 146; Percent complete: 14.6%; Average loss: 2.3231\n","Iteration: 147; Percent complete: 14.7%; Average loss: 2.2432\n","Iteration: 148; Percent complete: 14.8%; Average loss: 2.1725\n","Iteration: 149; Percent complete: 14.9%; Average loss: 2.4407\n","Iteration: 150; Percent complete: 15.0%; Average loss: 2.2746\n","Iteration: 151; Percent complete: 15.1%; Average loss: 2.1567\n","Iteration: 152; Percent complete: 15.2%; Average loss: 2.1887\n","Iteration: 153; Percent complete: 15.3%; Average loss: 2.4184\n","Iteration: 154; Percent complete: 15.4%; Average loss: 2.1557\n","Iteration: 155; Percent complete: 15.5%; Average loss: 2.4098\n","Iteration: 156; Percent complete: 15.6%; Average loss: 2.2770\n","Iteration: 157; Percent complete: 15.7%; Average loss: 2.2990\n","Iteration: 158; Percent complete: 15.8%; Average loss: 2.3093\n","Iteration: 159; Percent complete: 15.9%; Average loss: 2.1042\n","Iteration: 160; Percent complete: 16.0%; Average loss: 2.0502\n","Iteration: 161; Percent complete: 16.1%; Average loss: 2.1348\n","Iteration: 162; Percent complete: 16.2%; Average loss: 2.1213\n","Iteration: 163; Percent complete: 16.3%; Average loss: 2.1036\n","Iteration: 164; Percent complete: 16.4%; Average loss: 2.1712\n","Iteration: 165; Percent complete: 16.5%; Average loss: 1.9010\n","Iteration: 166; Percent complete: 16.6%; Average loss: 2.2332\n","Iteration: 167; Percent complete: 16.7%; Average loss: 2.1447\n","Iteration: 168; Percent complete: 16.8%; Average loss: 2.2226\n","Iteration: 169; Percent complete: 16.9%; Average loss: 2.2315\n","Iteration: 170; Percent complete: 17.0%; Average loss: 2.2870\n","Iteration: 171; Percent complete: 17.1%; Average loss: 2.1146\n","Iteration: 172; Percent complete: 17.2%; Average loss: 2.1814\n","Iteration: 173; Percent complete: 17.3%; Average loss: 2.1313\n","Iteration: 174; Percent complete: 17.4%; Average loss: 2.1631\n","Iteration: 175; Percent complete: 17.5%; Average loss: 2.1265\n","Iteration: 176; Percent complete: 17.6%; Average loss: 2.1226\n","Iteration: 177; Percent complete: 17.7%; Average loss: 2.2575\n","Iteration: 178; Percent complete: 17.8%; Average loss: 2.0235\n","Iteration: 179; Percent complete: 17.9%; Average loss: 2.0201\n","Iteration: 180; Percent complete: 18.0%; Average loss: 2.0517\n","Iteration: 181; Percent complete: 18.1%; Average loss: 2.2040\n","Iteration: 182; Percent complete: 18.2%; Average loss: 2.1197\n","Iteration: 183; Percent complete: 18.3%; Average loss: 1.8497\n","Iteration: 184; Percent complete: 18.4%; Average loss: 2.0154\n","Iteration: 185; Percent complete: 18.5%; Average loss: 2.0334\n","Iteration: 186; Percent complete: 18.6%; Average loss: 2.0552\n","Iteration: 187; Percent complete: 18.7%; Average loss: 2.2068\n","Iteration: 188; Percent complete: 18.8%; Average loss: 2.0155\n","Iteration: 189; Percent complete: 18.9%; Average loss: 2.0373\n","Iteration: 190; Percent complete: 19.0%; Average loss: 2.3050\n","Iteration: 191; Percent complete: 19.1%; Average loss: 2.1074\n","Iteration: 192; Percent complete: 19.2%; Average loss: 2.1706\n","Iteration: 193; Percent complete: 19.3%; Average loss: 1.9142\n","Iteration: 194; Percent complete: 19.4%; Average loss: 2.1373\n","Iteration: 195; Percent complete: 19.5%; Average loss: 2.0347\n","Iteration: 196; Percent complete: 19.6%; Average loss: 2.0280\n","Iteration: 197; Percent complete: 19.7%; Average loss: 1.8990\n","Iteration: 198; Percent complete: 19.8%; Average loss: 1.9592\n","Iteration: 199; Percent complete: 19.9%; Average loss: 2.2089\n","Iteration: 200; Percent complete: 20.0%; Average loss: 1.9776\n","Iteration: 201; Percent complete: 20.1%; Average loss: 2.0082\n","Iteration: 202; Percent complete: 20.2%; Average loss: 1.9252\n","Iteration: 203; Percent complete: 20.3%; Average loss: 1.8063\n","Iteration: 204; Percent complete: 20.4%; Average loss: 1.9277\n","Iteration: 205; Percent complete: 20.5%; Average loss: 1.8508\n","Iteration: 206; Percent complete: 20.6%; Average loss: 2.1074\n","Iteration: 207; Percent complete: 20.7%; Average loss: 2.0063\n","Iteration: 208; Percent complete: 20.8%; Average loss: 2.0000\n","Iteration: 209; Percent complete: 20.9%; Average loss: 1.8699\n","Iteration: 210; Percent complete: 21.0%; Average loss: 1.6801\n","Iteration: 211; Percent complete: 21.1%; Average loss: 1.9039\n","Iteration: 212; Percent complete: 21.2%; Average loss: 1.7437\n","Iteration: 213; Percent complete: 21.3%; Average loss: 2.0382\n","Iteration: 214; Percent complete: 21.4%; Average loss: 1.8792\n","Iteration: 215; Percent complete: 21.5%; Average loss: 1.9676\n","Iteration: 216; Percent complete: 21.6%; Average loss: 1.9235\n","Iteration: 217; Percent complete: 21.7%; Average loss: 1.9426\n","Iteration: 218; Percent complete: 21.8%; Average loss: 1.9598\n","Iteration: 219; Percent complete: 21.9%; Average loss: 1.8480\n","Iteration: 220; Percent complete: 22.0%; Average loss: 1.8401\n","Iteration: 221; Percent complete: 22.1%; Average loss: 1.7789\n","Iteration: 222; Percent complete: 22.2%; Average loss: 1.9721\n","Iteration: 223; Percent complete: 22.3%; Average loss: 1.8556\n","Iteration: 224; Percent complete: 22.4%; Average loss: 1.7669\n","Iteration: 225; Percent complete: 22.5%; Average loss: 1.7578\n","Iteration: 226; Percent complete: 22.6%; Average loss: 1.9635\n","Iteration: 227; Percent complete: 22.7%; Average loss: 1.7565\n","Iteration: 228; Percent complete: 22.8%; Average loss: 1.7087\n","Iteration: 229; Percent complete: 22.9%; Average loss: 1.7879\n","Iteration: 230; Percent complete: 23.0%; Average loss: 1.6350\n","Iteration: 231; Percent complete: 23.1%; Average loss: 1.9072\n","Iteration: 232; Percent complete: 23.2%; Average loss: 1.8580\n","Iteration: 233; Percent complete: 23.3%; Average loss: 1.5976\n","Iteration: 234; Percent complete: 23.4%; Average loss: 1.6847\n","Iteration: 235; Percent complete: 23.5%; Average loss: 1.6570\n","Iteration: 236; Percent complete: 23.6%; Average loss: 1.5801\n","Iteration: 237; Percent complete: 23.7%; Average loss: 1.5632\n","Iteration: 238; Percent complete: 23.8%; Average loss: 1.7295\n","Iteration: 239; Percent complete: 23.9%; Average loss: 1.7005\n","Iteration: 240; Percent complete: 24.0%; Average loss: 1.6246\n","Iteration: 241; Percent complete: 24.1%; Average loss: 1.8040\n","Iteration: 242; Percent complete: 24.2%; Average loss: 1.8001\n","Iteration: 243; Percent complete: 24.3%; Average loss: 1.7809\n","Iteration: 244; Percent complete: 24.4%; Average loss: 1.6939\n","Iteration: 245; Percent complete: 24.5%; Average loss: 1.6237\n","Iteration: 246; Percent complete: 24.6%; Average loss: 1.7993\n","Iteration: 247; Percent complete: 24.7%; Average loss: 1.5990\n","Iteration: 248; Percent complete: 24.8%; Average loss: 1.5660\n","Iteration: 249; Percent complete: 24.9%; Average loss: 1.8326\n","Iteration: 250; Percent complete: 25.0%; Average loss: 1.6388\n","Iteration: 251; Percent complete: 25.1%; Average loss: 1.6672\n","Iteration: 252; Percent complete: 25.2%; Average loss: 1.5932\n","Iteration: 253; Percent complete: 25.3%; Average loss: 1.5702\n","Iteration: 254; Percent complete: 25.4%; Average loss: 1.5205\n","Iteration: 255; Percent complete: 25.5%; Average loss: 1.6838\n","Iteration: 256; Percent complete: 25.6%; Average loss: 1.5252\n","Iteration: 257; Percent complete: 25.7%; Average loss: 1.6789\n","Iteration: 258; Percent complete: 25.8%; Average loss: 1.5975\n","Iteration: 259; Percent complete: 25.9%; Average loss: 1.4613\n","Iteration: 260; Percent complete: 26.0%; Average loss: 1.4753\n","Iteration: 261; Percent complete: 26.1%; Average loss: 1.5605\n","Iteration: 262; Percent complete: 26.2%; Average loss: 1.4650\n","Iteration: 263; Percent complete: 26.3%; Average loss: 1.5390\n","Iteration: 264; Percent complete: 26.4%; Average loss: 1.5616\n","Iteration: 265; Percent complete: 26.5%; Average loss: 1.3980\n","Iteration: 266; Percent complete: 26.6%; Average loss: 1.6178\n","Iteration: 267; Percent complete: 26.7%; Average loss: 1.4113\n","Iteration: 268; Percent complete: 26.8%; Average loss: 1.7080\n","Iteration: 269; Percent complete: 26.9%; Average loss: 1.6244\n","Iteration: 270; Percent complete: 27.0%; Average loss: 1.4022\n","Iteration: 271; Percent complete: 27.1%; Average loss: 1.5726\n","Iteration: 272; Percent complete: 27.2%; Average loss: 1.5147\n","Iteration: 273; Percent complete: 27.3%; Average loss: 1.4619\n","Iteration: 274; Percent complete: 27.4%; Average loss: 1.6198\n","Iteration: 275; Percent complete: 27.5%; Average loss: 1.3830\n","Iteration: 276; Percent complete: 27.6%; Average loss: 1.5419\n","Iteration: 277; Percent complete: 27.7%; Average loss: 1.4924\n","Iteration: 278; Percent complete: 27.8%; Average loss: 1.3520\n","Iteration: 279; Percent complete: 27.9%; Average loss: 1.3529\n","Iteration: 280; Percent complete: 28.0%; Average loss: 1.4692\n","Iteration: 281; Percent complete: 28.1%; Average loss: 1.5084\n","Iteration: 282; Percent complete: 28.2%; Average loss: 1.3473\n","Iteration: 283; Percent complete: 28.3%; Average loss: 1.4781\n","Iteration: 284; Percent complete: 28.4%; Average loss: 1.3042\n","Iteration: 285; Percent complete: 28.5%; Average loss: 1.3324\n","Iteration: 286; Percent complete: 28.6%; Average loss: 1.3987\n","Iteration: 287; Percent complete: 28.7%; Average loss: 1.4217\n","Iteration: 288; Percent complete: 28.8%; Average loss: 1.3944\n","Iteration: 289; Percent complete: 28.9%; Average loss: 1.3313\n","Iteration: 290; Percent complete: 29.0%; Average loss: 1.4485\n","Iteration: 291; Percent complete: 29.1%; Average loss: 1.2092\n","Iteration: 292; Percent complete: 29.2%; Average loss: 1.3274\n","Iteration: 293; Percent complete: 29.3%; Average loss: 1.3678\n","Iteration: 294; Percent complete: 29.4%; Average loss: 1.2471\n","Iteration: 295; Percent complete: 29.5%; Average loss: 1.2942\n","Iteration: 296; Percent complete: 29.6%; Average loss: 1.4333\n","Iteration: 297; Percent complete: 29.7%; Average loss: 1.2609\n","Iteration: 298; Percent complete: 29.8%; Average loss: 1.3592\n","Iteration: 299; Percent complete: 29.9%; Average loss: 1.2195\n","Iteration: 300; Percent complete: 30.0%; Average loss: 1.2838\n","Iteration: 301; Percent complete: 30.1%; Average loss: 1.3820\n","Iteration: 302; Percent complete: 30.2%; Average loss: 1.3824\n","Iteration: 303; Percent complete: 30.3%; Average loss: 1.1836\n","Iteration: 304; Percent complete: 30.4%; Average loss: 1.2738\n","Iteration: 305; Percent complete: 30.5%; Average loss: 1.4048\n","Iteration: 306; Percent complete: 30.6%; Average loss: 1.3261\n","Iteration: 307; Percent complete: 30.7%; Average loss: 1.2449\n","Iteration: 308; Percent complete: 30.8%; Average loss: 1.2958\n","Iteration: 309; Percent complete: 30.9%; Average loss: 1.1575\n","Iteration: 310; Percent complete: 31.0%; Average loss: 1.1701\n","Iteration: 311; Percent complete: 31.1%; Average loss: 1.2721\n","Iteration: 312; Percent complete: 31.2%; Average loss: 1.2244\n","Iteration: 313; Percent complete: 31.3%; Average loss: 1.2914\n","Iteration: 314; Percent complete: 31.4%; Average loss: 1.1149\n","Iteration: 315; Percent complete: 31.5%; Average loss: 1.2735\n","Iteration: 316; Percent complete: 31.6%; Average loss: 1.1694\n","Iteration: 317; Percent complete: 31.7%; Average loss: 1.1489\n","Iteration: 318; Percent complete: 31.8%; Average loss: 1.2256\n","Iteration: 319; Percent complete: 31.9%; Average loss: 1.2225\n","Iteration: 320; Percent complete: 32.0%; Average loss: 1.2459\n","Iteration: 321; Percent complete: 32.1%; Average loss: 1.3008\n","Iteration: 322; Percent complete: 32.2%; Average loss: 1.1420\n","Iteration: 323; Percent complete: 32.3%; Average loss: 1.1321\n","Iteration: 324; Percent complete: 32.4%; Average loss: 1.1665\n","Iteration: 325; Percent complete: 32.5%; Average loss: 1.0499\n","Iteration: 326; Percent complete: 32.6%; Average loss: 1.0742\n","Iteration: 327; Percent complete: 32.7%; Average loss: 1.1100\n","Iteration: 328; Percent complete: 32.8%; Average loss: 1.2883\n","Iteration: 329; Percent complete: 32.9%; Average loss: 1.1225\n","Iteration: 330; Percent complete: 33.0%; Average loss: 1.1195\n","Iteration: 331; Percent complete: 33.1%; Average loss: 1.0862\n","Iteration: 332; Percent complete: 33.2%; Average loss: 1.2425\n","Iteration: 333; Percent complete: 33.3%; Average loss: 1.1496\n","Iteration: 334; Percent complete: 33.4%; Average loss: 1.2723\n","Iteration: 335; Percent complete: 33.5%; Average loss: 1.0170\n","Iteration: 336; Percent complete: 33.6%; Average loss: 1.0736\n","Iteration: 337; Percent complete: 33.7%; Average loss: 0.9938\n","Iteration: 338; Percent complete: 33.8%; Average loss: 1.1165\n","Iteration: 339; Percent complete: 33.9%; Average loss: 1.0479\n","Iteration: 340; Percent complete: 34.0%; Average loss: 1.0552\n","Iteration: 341; Percent complete: 34.1%; Average loss: 1.0473\n","Iteration: 342; Percent complete: 34.2%; Average loss: 1.0769\n","Iteration: 343; Percent complete: 34.3%; Average loss: 0.9542\n","Iteration: 344; Percent complete: 34.4%; Average loss: 1.0802\n","Iteration: 345; Percent complete: 34.5%; Average loss: 0.9412\n","Iteration: 346; Percent complete: 34.6%; Average loss: 0.9227\n","Iteration: 347; Percent complete: 34.7%; Average loss: 0.9832\n","Iteration: 348; Percent complete: 34.8%; Average loss: 0.9396\n","Iteration: 349; Percent complete: 34.9%; Average loss: 0.9402\n","Iteration: 350; Percent complete: 35.0%; Average loss: 1.0396\n","Iteration: 351; Percent complete: 35.1%; Average loss: 0.8383\n","Iteration: 352; Percent complete: 35.2%; Average loss: 0.9732\n","Iteration: 353; Percent complete: 35.3%; Average loss: 0.9266\n","Iteration: 354; Percent complete: 35.4%; Average loss: 1.1015\n","Iteration: 355; Percent complete: 35.5%; Average loss: 0.9436\n","Iteration: 356; Percent complete: 35.6%; Average loss: 0.9675\n","Iteration: 357; Percent complete: 35.7%; Average loss: 0.9209\n","Iteration: 358; Percent complete: 35.8%; Average loss: 1.0206\n","Iteration: 359; Percent complete: 35.9%; Average loss: 0.9342\n","Iteration: 360; Percent complete: 36.0%; Average loss: 1.0286\n","Iteration: 361; Percent complete: 36.1%; Average loss: 0.8915\n","Iteration: 362; Percent complete: 36.2%; Average loss: 0.9583\n","Iteration: 363; Percent complete: 36.3%; Average loss: 0.8418\n","Iteration: 364; Percent complete: 36.4%; Average loss: 0.8290\n","Iteration: 365; Percent complete: 36.5%; Average loss: 0.8864\n","Iteration: 366; Percent complete: 36.6%; Average loss: 0.8169\n","Iteration: 367; Percent complete: 36.7%; Average loss: 0.8385\n","Iteration: 368; Percent complete: 36.8%; Average loss: 0.8510\n","Iteration: 369; Percent complete: 36.9%; Average loss: 0.9342\n","Iteration: 370; Percent complete: 37.0%; Average loss: 0.8482\n","Iteration: 371; Percent complete: 37.1%; Average loss: 0.8360\n","Iteration: 372; Percent complete: 37.2%; Average loss: 0.8294\n","Iteration: 373; Percent complete: 37.3%; Average loss: 0.8350\n","Iteration: 374; Percent complete: 37.4%; Average loss: 0.9038\n","Iteration: 375; Percent complete: 37.5%; Average loss: 0.7961\n","Iteration: 376; Percent complete: 37.6%; Average loss: 0.8006\n","Iteration: 377; Percent complete: 37.7%; Average loss: 0.8883\n","Iteration: 378; Percent complete: 37.8%; Average loss: 0.8753\n","Iteration: 379; Percent complete: 37.9%; Average loss: 0.8214\n","Iteration: 380; Percent complete: 38.0%; Average loss: 0.9582\n","Iteration: 381; Percent complete: 38.1%; Average loss: 0.9331\n","Iteration: 382; Percent complete: 38.2%; Average loss: 0.8742\n","Iteration: 383; Percent complete: 38.3%; Average loss: 0.7365\n","Iteration: 384; Percent complete: 38.4%; Average loss: 0.9380\n","Iteration: 385; Percent complete: 38.5%; Average loss: 0.8225\n","Iteration: 386; Percent complete: 38.6%; Average loss: 0.8933\n","Iteration: 387; Percent complete: 38.7%; Average loss: 0.6986\n","Iteration: 388; Percent complete: 38.8%; Average loss: 0.7667\n","Iteration: 389; Percent complete: 38.9%; Average loss: 0.7691\n","Iteration: 390; Percent complete: 39.0%; Average loss: 0.7080\n","Iteration: 391; Percent complete: 39.1%; Average loss: 0.7993\n","Iteration: 392; Percent complete: 39.2%; Average loss: 0.7626\n","Iteration: 393; Percent complete: 39.3%; Average loss: 0.8686\n","Iteration: 394; Percent complete: 39.4%; Average loss: 0.6955\n","Iteration: 395; Percent complete: 39.5%; Average loss: 0.8119\n","Iteration: 396; Percent complete: 39.6%; Average loss: 0.7167\n","Iteration: 397; Percent complete: 39.7%; Average loss: 0.8190\n","Iteration: 398; Percent complete: 39.8%; Average loss: 0.8544\n","Iteration: 399; Percent complete: 39.9%; Average loss: 0.6915\n","Iteration: 400; Percent complete: 40.0%; Average loss: 0.8081\n","Iteration: 401; Percent complete: 40.1%; Average loss: 0.7572\n","Iteration: 402; Percent complete: 40.2%; Average loss: 0.6568\n","Iteration: 403; Percent complete: 40.3%; Average loss: 0.7904\n","Iteration: 404; Percent complete: 40.4%; Average loss: 0.6944\n","Iteration: 405; Percent complete: 40.5%; Average loss: 0.7080\n","Iteration: 406; Percent complete: 40.6%; Average loss: 0.6913\n","Iteration: 407; Percent complete: 40.7%; Average loss: 0.7637\n","Iteration: 408; Percent complete: 40.8%; Average loss: 0.6661\n","Iteration: 409; Percent complete: 40.9%; Average loss: 0.8734\n","Iteration: 410; Percent complete: 41.0%; Average loss: 0.6594\n","Iteration: 411; Percent complete: 41.1%; Average loss: 0.6684\n","Iteration: 412; Percent complete: 41.2%; Average loss: 0.7044\n","Iteration: 413; Percent complete: 41.3%; Average loss: 0.7102\n","Iteration: 414; Percent complete: 41.4%; Average loss: 0.6798\n","Iteration: 415; Percent complete: 41.5%; Average loss: 0.7089\n","Iteration: 416; Percent complete: 41.6%; Average loss: 0.7532\n","Iteration: 417; Percent complete: 41.7%; Average loss: 0.6759\n","Iteration: 418; Percent complete: 41.8%; Average loss: 0.6428\n","Iteration: 419; Percent complete: 41.9%; Average loss: 0.5732\n","Iteration: 420; Percent complete: 42.0%; Average loss: 0.5949\n","Iteration: 421; Percent complete: 42.1%; Average loss: 0.6619\n","Iteration: 422; Percent complete: 42.2%; Average loss: 0.6352\n","Iteration: 423; Percent complete: 42.3%; Average loss: 0.7313\n","Iteration: 424; Percent complete: 42.4%; Average loss: 0.6622\n","Iteration: 425; Percent complete: 42.5%; Average loss: 0.5958\n","Iteration: 426; Percent complete: 42.6%; Average loss: 0.5695\n","Iteration: 427; Percent complete: 42.7%; Average loss: 0.5748\n","Iteration: 428; Percent complete: 42.8%; Average loss: 0.6575\n","Iteration: 429; Percent complete: 42.9%; Average loss: 0.5760\n","Iteration: 430; Percent complete: 43.0%; Average loss: 0.5359\n","Iteration: 431; Percent complete: 43.1%; Average loss: 0.5832\n","Iteration: 432; Percent complete: 43.2%; Average loss: 0.5928\n","Iteration: 433; Percent complete: 43.3%; Average loss: 0.6678\n","Iteration: 434; Percent complete: 43.4%; Average loss: 0.5451\n","Iteration: 435; Percent complete: 43.5%; Average loss: 0.6423\n","Iteration: 436; Percent complete: 43.6%; Average loss: 0.5679\n","Iteration: 437; Percent complete: 43.7%; Average loss: 0.5221\n","Iteration: 438; Percent complete: 43.8%; Average loss: 0.5543\n","Iteration: 439; Percent complete: 43.9%; Average loss: 0.5730\n","Iteration: 440; Percent complete: 44.0%; Average loss: 0.6876\n","Iteration: 441; Percent complete: 44.1%; Average loss: 0.5907\n","Iteration: 442; Percent complete: 44.2%; Average loss: 0.5080\n","Iteration: 443; Percent complete: 44.3%; Average loss: 0.5190\n","Iteration: 444; Percent complete: 44.4%; Average loss: 0.5169\n","Iteration: 445; Percent complete: 44.5%; Average loss: 0.5587\n","Iteration: 446; Percent complete: 44.6%; Average loss: 0.4713\n","Iteration: 447; Percent complete: 44.7%; Average loss: 0.5843\n","Iteration: 448; Percent complete: 44.8%; Average loss: 0.5152\n","Iteration: 449; Percent complete: 44.9%; Average loss: 0.6441\n","Iteration: 450; Percent complete: 45.0%; Average loss: 0.5567\n","Iteration: 451; Percent complete: 45.1%; Average loss: 0.5696\n","Iteration: 452; Percent complete: 45.2%; Average loss: 0.4840\n","Iteration: 453; Percent complete: 45.3%; Average loss: 0.5725\n","Iteration: 454; Percent complete: 45.4%; Average loss: 0.4926\n","Iteration: 455; Percent complete: 45.5%; Average loss: 0.5140\n","Iteration: 456; Percent complete: 45.6%; Average loss: 0.4881\n","Iteration: 457; Percent complete: 45.7%; Average loss: 0.4319\n","Iteration: 458; Percent complete: 45.8%; Average loss: 0.4915\n","Iteration: 459; Percent complete: 45.9%; Average loss: 0.4647\n","Iteration: 460; Percent complete: 46.0%; Average loss: 0.4856\n","Iteration: 461; Percent complete: 46.1%; Average loss: 0.5203\n","Iteration: 462; Percent complete: 46.2%; Average loss: 0.5520\n","Iteration: 463; Percent complete: 46.3%; Average loss: 0.5600\n","Iteration: 464; Percent complete: 46.4%; Average loss: 0.4388\n","Iteration: 465; Percent complete: 46.5%; Average loss: 0.4450\n","Iteration: 466; Percent complete: 46.6%; Average loss: 0.4587\n","Iteration: 467; Percent complete: 46.7%; Average loss: 0.4453\n","Iteration: 468; Percent complete: 46.8%; Average loss: 0.5798\n","Iteration: 469; Percent complete: 46.9%; Average loss: 0.4623\n","Iteration: 470; Percent complete: 47.0%; Average loss: 0.4079\n","Iteration: 471; Percent complete: 47.1%; Average loss: 0.4380\n","Iteration: 472; Percent complete: 47.2%; Average loss: 0.4613\n","Iteration: 473; Percent complete: 47.3%; Average loss: 0.3828\n","Iteration: 474; Percent complete: 47.4%; Average loss: 0.4596\n","Iteration: 475; Percent complete: 47.5%; Average loss: 0.4048\n","Iteration: 476; Percent complete: 47.6%; Average loss: 0.3880\n","Iteration: 477; Percent complete: 47.7%; Average loss: 0.4567\n","Iteration: 478; Percent complete: 47.8%; Average loss: 0.3876\n","Iteration: 479; Percent complete: 47.9%; Average loss: 0.3715\n","Iteration: 480; Percent complete: 48.0%; Average loss: 0.3734\n","Iteration: 481; Percent complete: 48.1%; Average loss: 0.4359\n","Iteration: 482; Percent complete: 48.2%; Average loss: 0.3989\n","Iteration: 483; Percent complete: 48.3%; Average loss: 0.4553\n","Iteration: 484; Percent complete: 48.4%; Average loss: 0.3881\n","Iteration: 485; Percent complete: 48.5%; Average loss: 0.4044\n","Iteration: 486; Percent complete: 48.6%; Average loss: 0.3851\n","Iteration: 487; Percent complete: 48.7%; Average loss: 0.3184\n","Iteration: 488; Percent complete: 48.8%; Average loss: 0.4254\n","Iteration: 489; Percent complete: 48.9%; Average loss: 0.3785\n","Iteration: 490; Percent complete: 49.0%; Average loss: 0.4575\n","Iteration: 491; Percent complete: 49.1%; Average loss: 0.3913\n","Iteration: 492; Percent complete: 49.2%; Average loss: 0.3566\n","Iteration: 493; Percent complete: 49.3%; Average loss: 0.3741\n","Iteration: 494; Percent complete: 49.4%; Average loss: 0.3325\n","Iteration: 495; Percent complete: 49.5%; Average loss: 0.3333\n","Iteration: 496; Percent complete: 49.6%; Average loss: 0.3648\n","Iteration: 497; Percent complete: 49.7%; Average loss: 0.3609\n","Iteration: 498; Percent complete: 49.8%; Average loss: 0.3522\n","Iteration: 499; Percent complete: 49.9%; Average loss: 0.3341\n","Iteration: 500; Percent complete: 50.0%; Average loss: 0.2977\n","Iteration: 501; Percent complete: 50.1%; Average loss: 0.3882\n","Iteration: 502; Percent complete: 50.2%; Average loss: 0.4556\n","Iteration: 503; Percent complete: 50.3%; Average loss: 0.4316\n","Iteration: 504; Percent complete: 50.4%; Average loss: 0.3809\n","Iteration: 505; Percent complete: 50.5%; Average loss: 0.3518\n","Iteration: 506; Percent complete: 50.6%; Average loss: 0.3370\n","Iteration: 507; Percent complete: 50.7%; Average loss: 0.3838\n","Iteration: 508; Percent complete: 50.8%; Average loss: 0.4105\n","Iteration: 509; Percent complete: 50.9%; Average loss: 0.3237\n","Iteration: 510; Percent complete: 51.0%; Average loss: 0.3252\n","Iteration: 511; Percent complete: 51.1%; Average loss: 0.2832\n","Iteration: 512; Percent complete: 51.2%; Average loss: 0.3553\n","Iteration: 513; Percent complete: 51.3%; Average loss: 0.3853\n","Iteration: 514; Percent complete: 51.4%; Average loss: 0.2910\n","Iteration: 515; Percent complete: 51.5%; Average loss: 0.3332\n","Iteration: 516; Percent complete: 51.6%; Average loss: 0.2892\n","Iteration: 517; Percent complete: 51.7%; Average loss: 0.3199\n","Iteration: 518; Percent complete: 51.8%; Average loss: 0.3279\n","Iteration: 519; Percent complete: 51.9%; Average loss: 0.3096\n","Iteration: 520; Percent complete: 52.0%; Average loss: 0.2810\n","Iteration: 521; Percent complete: 52.1%; Average loss: 0.3119\n","Iteration: 522; Percent complete: 52.2%; Average loss: 0.3152\n","Iteration: 523; Percent complete: 52.3%; Average loss: 0.2956\n","Iteration: 524; Percent complete: 52.4%; Average loss: 0.3138\n","Iteration: 525; Percent complete: 52.5%; Average loss: 0.3530\n","Iteration: 526; Percent complete: 52.6%; Average loss: 0.3092\n","Iteration: 527; Percent complete: 52.7%; Average loss: 0.2346\n","Iteration: 528; Percent complete: 52.8%; Average loss: 0.3067\n","Iteration: 529; Percent complete: 52.9%; Average loss: 0.2816\n","Iteration: 530; Percent complete: 53.0%; Average loss: 0.3214\n","Iteration: 531; Percent complete: 53.1%; Average loss: 0.3036\n","Iteration: 532; Percent complete: 53.2%; Average loss: 0.2778\n","Iteration: 533; Percent complete: 53.3%; Average loss: 0.2674\n","Iteration: 534; Percent complete: 53.4%; Average loss: 0.2795\n","Iteration: 535; Percent complete: 53.5%; Average loss: 0.2470\n","Iteration: 536; Percent complete: 53.6%; Average loss: 0.2649\n","Iteration: 537; Percent complete: 53.7%; Average loss: 0.3378\n","Iteration: 538; Percent complete: 53.8%; Average loss: 0.2882\n","Iteration: 539; Percent complete: 53.9%; Average loss: 0.2477\n","Iteration: 540; Percent complete: 54.0%; Average loss: 0.2846\n","Iteration: 541; Percent complete: 54.1%; Average loss: 0.2826\n","Iteration: 542; Percent complete: 54.2%; Average loss: 0.2471\n","Iteration: 543; Percent complete: 54.3%; Average loss: 0.3358\n","Iteration: 544; Percent complete: 54.4%; Average loss: 0.3096\n","Iteration: 545; Percent complete: 54.5%; Average loss: 0.2671\n","Iteration: 546; Percent complete: 54.6%; Average loss: 0.2768\n","Iteration: 547; Percent complete: 54.7%; Average loss: 0.3156\n","Iteration: 548; Percent complete: 54.8%; Average loss: 0.2979\n","Iteration: 549; Percent complete: 54.9%; Average loss: 0.2807\n","Iteration: 550; Percent complete: 55.0%; Average loss: 0.2633\n","Iteration: 551; Percent complete: 55.1%; Average loss: 0.2656\n","Iteration: 552; Percent complete: 55.2%; Average loss: 0.2539\n","Iteration: 553; Percent complete: 55.3%; Average loss: 0.2398\n","Iteration: 554; Percent complete: 55.4%; Average loss: 0.2204\n","Iteration: 555; Percent complete: 55.5%; Average loss: 0.2408\n","Iteration: 556; Percent complete: 55.6%; Average loss: 0.2625\n","Iteration: 557; Percent complete: 55.7%; Average loss: 0.2748\n","Iteration: 558; Percent complete: 55.8%; Average loss: 0.2430\n","Iteration: 559; Percent complete: 55.9%; Average loss: 0.2643\n","Iteration: 560; Percent complete: 56.0%; Average loss: 0.2617\n","Iteration: 561; Percent complete: 56.1%; Average loss: 0.2308\n","Iteration: 562; Percent complete: 56.2%; Average loss: 0.2363\n","Iteration: 563; Percent complete: 56.3%; Average loss: 0.2435\n","Iteration: 564; Percent complete: 56.4%; Average loss: 0.2163\n","Iteration: 565; Percent complete: 56.5%; Average loss: 0.2468\n","Iteration: 566; Percent complete: 56.6%; Average loss: 0.2077\n","Iteration: 567; Percent complete: 56.7%; Average loss: 0.2094\n","Iteration: 568; Percent complete: 56.8%; Average loss: 0.2393\n","Iteration: 569; Percent complete: 56.9%; Average loss: 0.2700\n","Iteration: 570; Percent complete: 57.0%; Average loss: 0.2393\n","Iteration: 571; Percent complete: 57.1%; Average loss: 0.2291\n","Iteration: 572; Percent complete: 57.2%; Average loss: 0.2299\n","Iteration: 573; Percent complete: 57.3%; Average loss: 0.2380\n","Iteration: 574; Percent complete: 57.4%; Average loss: 0.2328\n","Iteration: 575; Percent complete: 57.5%; Average loss: 0.2116\n","Iteration: 576; Percent complete: 57.6%; Average loss: 0.1966\n","Iteration: 577; Percent complete: 57.7%; Average loss: 0.1782\n","Iteration: 578; Percent complete: 57.8%; Average loss: 0.2047\n","Iteration: 579; Percent complete: 57.9%; Average loss: 0.2326\n","Iteration: 580; Percent complete: 58.0%; Average loss: 0.1726\n","Iteration: 581; Percent complete: 58.1%; Average loss: 0.2162\n","Iteration: 582; Percent complete: 58.2%; Average loss: 0.2264\n","Iteration: 583; Percent complete: 58.3%; Average loss: 0.1977\n","Iteration: 584; Percent complete: 58.4%; Average loss: 0.1962\n","Iteration: 585; Percent complete: 58.5%; Average loss: 0.2021\n","Iteration: 586; Percent complete: 58.6%; Average loss: 0.2146\n","Iteration: 587; Percent complete: 58.7%; Average loss: 0.1672\n","Iteration: 588; Percent complete: 58.8%; Average loss: 0.1661\n","Iteration: 589; Percent complete: 58.9%; Average loss: 0.1788\n","Iteration: 590; Percent complete: 59.0%; Average loss: 0.1920\n","Iteration: 591; Percent complete: 59.1%; Average loss: 0.2127\n","Iteration: 592; Percent complete: 59.2%; Average loss: 0.1890\n","Iteration: 593; Percent complete: 59.3%; Average loss: 0.1866\n","Iteration: 594; Percent complete: 59.4%; Average loss: 0.2013\n","Iteration: 595; Percent complete: 59.5%; Average loss: 0.1739\n","Iteration: 596; Percent complete: 59.6%; Average loss: 0.1897\n","Iteration: 597; Percent complete: 59.7%; Average loss: 0.1540\n","Iteration: 598; Percent complete: 59.8%; Average loss: 0.1543\n","Iteration: 599; Percent complete: 59.9%; Average loss: 0.1839\n","Iteration: 600; Percent complete: 60.0%; Average loss: 0.2084\n","Iteration: 601; Percent complete: 60.1%; Average loss: 0.1835\n","Iteration: 602; Percent complete: 60.2%; Average loss: 0.1826\n","Iteration: 603; Percent complete: 60.3%; Average loss: 0.1721\n","Iteration: 604; Percent complete: 60.4%; Average loss: 0.1944\n","Iteration: 605; Percent complete: 60.5%; Average loss: 0.1750\n","Iteration: 606; Percent complete: 60.6%; Average loss: 0.1398\n","Iteration: 607; Percent complete: 60.7%; Average loss: 0.1892\n","Iteration: 608; Percent complete: 60.8%; Average loss: 0.1742\n","Iteration: 609; Percent complete: 60.9%; Average loss: 0.1773\n","Iteration: 610; Percent complete: 61.0%; Average loss: 0.1653\n","Iteration: 611; Percent complete: 61.1%; Average loss: 0.1727\n","Iteration: 612; Percent complete: 61.2%; Average loss: 0.1625\n","Iteration: 613; Percent complete: 61.3%; Average loss: 0.1472\n","Iteration: 614; Percent complete: 61.4%; Average loss: 0.1598\n","Iteration: 615; Percent complete: 61.5%; Average loss: 0.1786\n","Iteration: 616; Percent complete: 61.6%; Average loss: 0.1335\n","Iteration: 617; Percent complete: 61.7%; Average loss: 0.1386\n","Iteration: 618; Percent complete: 61.8%; Average loss: 0.1270\n","Iteration: 619; Percent complete: 61.9%; Average loss: 0.1442\n","Iteration: 620; Percent complete: 62.0%; Average loss: 0.1394\n","Iteration: 621; Percent complete: 62.1%; Average loss: 0.1403\n","Iteration: 622; Percent complete: 62.2%; Average loss: 0.1414\n","Iteration: 623; Percent complete: 62.3%; Average loss: 0.1438\n","Iteration: 624; Percent complete: 62.4%; Average loss: 0.1331\n","Iteration: 625; Percent complete: 62.5%; Average loss: 0.1723\n","Iteration: 626; Percent complete: 62.6%; Average loss: 0.1365\n","Iteration: 627; Percent complete: 62.7%; Average loss: 0.1212\n","Iteration: 628; Percent complete: 62.8%; Average loss: 0.1283\n","Iteration: 629; Percent complete: 62.9%; Average loss: 0.1779\n","Iteration: 630; Percent complete: 63.0%; Average loss: 0.1369\n","Iteration: 631; Percent complete: 63.1%; Average loss: 0.1494\n","Iteration: 632; Percent complete: 63.2%; Average loss: 0.1282\n","Iteration: 633; Percent complete: 63.3%; Average loss: 0.1249\n","Iteration: 634; Percent complete: 63.4%; Average loss: 0.1062\n","Iteration: 635; Percent complete: 63.5%; Average loss: 0.1499\n","Iteration: 636; Percent complete: 63.6%; Average loss: 0.1497\n","Iteration: 637; Percent complete: 63.7%; Average loss: 0.1308\n","Iteration: 638; Percent complete: 63.8%; Average loss: 0.1277\n","Iteration: 639; Percent complete: 63.9%; Average loss: 0.1419\n","Iteration: 640; Percent complete: 64.0%; Average loss: 0.1517\n","Iteration: 641; Percent complete: 64.1%; Average loss: 0.1324\n","Iteration: 642; Percent complete: 64.2%; Average loss: 0.1355\n","Iteration: 643; Percent complete: 64.3%; Average loss: 0.1119\n","Iteration: 644; Percent complete: 64.4%; Average loss: 0.1291\n","Iteration: 645; Percent complete: 64.5%; Average loss: 0.1296\n","Iteration: 646; Percent complete: 64.6%; Average loss: 0.1015\n","Iteration: 647; Percent complete: 64.7%; Average loss: 0.1347\n","Iteration: 648; Percent complete: 64.8%; Average loss: 0.1391\n","Iteration: 649; Percent complete: 64.9%; Average loss: 0.1211\n","Iteration: 650; Percent complete: 65.0%; Average loss: 0.1303\n","Iteration: 651; Percent complete: 65.1%; Average loss: 0.1393\n","Iteration: 652; Percent complete: 65.2%; Average loss: 0.1232\n","Iteration: 653; Percent complete: 65.3%; Average loss: 0.1124\n","Iteration: 654; Percent complete: 65.4%; Average loss: 0.1142\n","Iteration: 655; Percent complete: 65.5%; Average loss: 0.1122\n","Iteration: 656; Percent complete: 65.6%; Average loss: 0.0949\n","Iteration: 657; Percent complete: 65.7%; Average loss: 0.1095\n","Iteration: 658; Percent complete: 65.8%; Average loss: 0.1275\n","Iteration: 659; Percent complete: 65.9%; Average loss: 0.1334\n","Iteration: 660; Percent complete: 66.0%; Average loss: 0.1317\n","Iteration: 661; Percent complete: 66.1%; Average loss: 0.1091\n","Iteration: 662; Percent complete: 66.2%; Average loss: 0.0998\n","Iteration: 663; Percent complete: 66.3%; Average loss: 0.1237\n","Iteration: 664; Percent complete: 66.4%; Average loss: 0.1101\n","Iteration: 665; Percent complete: 66.5%; Average loss: 0.1096\n","Iteration: 666; Percent complete: 66.6%; Average loss: 0.1199\n","Iteration: 667; Percent complete: 66.7%; Average loss: 0.0986\n","Iteration: 668; Percent complete: 66.8%; Average loss: 0.0967\n","Iteration: 669; Percent complete: 66.9%; Average loss: 0.0947\n","Iteration: 670; Percent complete: 67.0%; Average loss: 0.1068\n","Iteration: 671; Percent complete: 67.1%; Average loss: 0.1201\n","Iteration: 672; Percent complete: 67.2%; Average loss: 0.1041\n","Iteration: 673; Percent complete: 67.3%; Average loss: 0.0890\n","Iteration: 674; Percent complete: 67.4%; Average loss: 0.1066\n","Iteration: 675; Percent complete: 67.5%; Average loss: 0.1236\n","Iteration: 676; Percent complete: 67.6%; Average loss: 0.0971\n","Iteration: 677; Percent complete: 67.7%; Average loss: 0.1094\n","Iteration: 678; Percent complete: 67.8%; Average loss: 0.1016\n","Iteration: 679; Percent complete: 67.9%; Average loss: 0.1061\n","Iteration: 680; Percent complete: 68.0%; Average loss: 0.1000\n","Iteration: 681; Percent complete: 68.1%; Average loss: 0.0990\n","Iteration: 682; Percent complete: 68.2%; Average loss: 0.0994\n","Iteration: 683; Percent complete: 68.3%; Average loss: 0.1095\n","Iteration: 684; Percent complete: 68.4%; Average loss: 0.0864\n","Iteration: 685; Percent complete: 68.5%; Average loss: 0.0932\n","Iteration: 686; Percent complete: 68.6%; Average loss: 0.1027\n","Iteration: 687; Percent complete: 68.7%; Average loss: 0.0824\n","Iteration: 688; Percent complete: 68.8%; Average loss: 0.0853\n","Iteration: 689; Percent complete: 68.9%; Average loss: 0.0965\n","Iteration: 690; Percent complete: 69.0%; Average loss: 0.1001\n","Iteration: 691; Percent complete: 69.1%; Average loss: 0.0864\n","Iteration: 692; Percent complete: 69.2%; Average loss: 0.1082\n","Iteration: 693; Percent complete: 69.3%; Average loss: 0.1034\n","Iteration: 694; Percent complete: 69.4%; Average loss: 0.1081\n","Iteration: 695; Percent complete: 69.5%; Average loss: 0.0871\n","Iteration: 696; Percent complete: 69.6%; Average loss: 0.0951\n","Iteration: 697; Percent complete: 69.7%; Average loss: 0.0934\n","Iteration: 698; Percent complete: 69.8%; Average loss: 0.0953\n","Iteration: 699; Percent complete: 69.9%; Average loss: 0.0747\n","Iteration: 700; Percent complete: 70.0%; Average loss: 0.1157\n","Iteration: 701; Percent complete: 70.1%; Average loss: 0.0951\n","Iteration: 702; Percent complete: 70.2%; Average loss: 0.0907\n","Iteration: 703; Percent complete: 70.3%; Average loss: 0.0906\n","Iteration: 704; Percent complete: 70.4%; Average loss: 0.0834\n","Iteration: 705; Percent complete: 70.5%; Average loss: 0.0948\n","Iteration: 706; Percent complete: 70.6%; Average loss: 0.0888\n","Iteration: 707; Percent complete: 70.7%; Average loss: 0.0871\n","Iteration: 708; Percent complete: 70.8%; Average loss: 0.0883\n","Iteration: 709; Percent complete: 70.9%; Average loss: 0.0719\n","Iteration: 710; Percent complete: 71.0%; Average loss: 0.1007\n","Iteration: 711; Percent complete: 71.1%; Average loss: 0.0824\n","Iteration: 712; Percent complete: 71.2%; Average loss: 0.1022\n","Iteration: 713; Percent complete: 71.3%; Average loss: 0.0765\n","Iteration: 714; Percent complete: 71.4%; Average loss: 0.0839\n","Iteration: 715; Percent complete: 71.5%; Average loss: 0.0728\n","Iteration: 716; Percent complete: 71.6%; Average loss: 0.0810\n","Iteration: 717; Percent complete: 71.7%; Average loss: 0.0791\n","Iteration: 718; Percent complete: 71.8%; Average loss: 0.0692\n","Iteration: 719; Percent complete: 71.9%; Average loss: 0.0868\n","Iteration: 720; Percent complete: 72.0%; Average loss: 0.0871\n","Iteration: 721; Percent complete: 72.1%; Average loss: 0.0720\n","Iteration: 722; Percent complete: 72.2%; Average loss: 0.0887\n","Iteration: 723; Percent complete: 72.3%; Average loss: 0.0782\n","Iteration: 724; Percent complete: 72.4%; Average loss: 0.0743\n","Iteration: 725; Percent complete: 72.5%; Average loss: 0.0853\n","Iteration: 726; Percent complete: 72.6%; Average loss: 0.0868\n","Iteration: 727; Percent complete: 72.7%; Average loss: 0.0833\n","Iteration: 728; Percent complete: 72.8%; Average loss: 0.0776\n","Iteration: 729; Percent complete: 72.9%; Average loss: 0.0825\n","Iteration: 730; Percent complete: 73.0%; Average loss: 0.0807\n","Iteration: 731; Percent complete: 73.1%; Average loss: 0.0779\n","Iteration: 732; Percent complete: 73.2%; Average loss: 0.0649\n","Iteration: 733; Percent complete: 73.3%; Average loss: 0.0911\n","Iteration: 734; Percent complete: 73.4%; Average loss: 0.0700\n","Iteration: 735; Percent complete: 73.5%; Average loss: 0.0694\n","Iteration: 736; Percent complete: 73.6%; Average loss: 0.0884\n","Iteration: 737; Percent complete: 73.7%; Average loss: 0.0707\n","Iteration: 738; Percent complete: 73.8%; Average loss: 0.0772\n","Iteration: 739; Percent complete: 73.9%; Average loss: 0.0762\n","Iteration: 740; Percent complete: 74.0%; Average loss: 0.0672\n","Iteration: 741; Percent complete: 74.1%; Average loss: 0.0756\n","Iteration: 742; Percent complete: 74.2%; Average loss: 0.0755\n","Iteration: 743; Percent complete: 74.3%; Average loss: 0.0663\n","Iteration: 744; Percent complete: 74.4%; Average loss: 0.0735\n","Iteration: 745; Percent complete: 74.5%; Average loss: 0.0692\n","Iteration: 746; Percent complete: 74.6%; Average loss: 0.0883\n","Iteration: 747; Percent complete: 74.7%; Average loss: 0.0659\n","Iteration: 748; Percent complete: 74.8%; Average loss: 0.0720\n","Iteration: 749; Percent complete: 74.9%; Average loss: 0.0684\n","Iteration: 750; Percent complete: 75.0%; Average loss: 0.0800\n","Iteration: 751; Percent complete: 75.1%; Average loss: 0.0914\n","Iteration: 752; Percent complete: 75.2%; Average loss: 0.0767\n","Iteration: 753; Percent complete: 75.3%; Average loss: 0.0719\n","Iteration: 754; Percent complete: 75.4%; Average loss: 0.0740\n","Iteration: 755; Percent complete: 75.5%; Average loss: 0.0896\n","Iteration: 756; Percent complete: 75.6%; Average loss: 0.0805\n","Iteration: 757; Percent complete: 75.7%; Average loss: 0.0962\n","Iteration: 758; Percent complete: 75.8%; Average loss: 0.0727\n","Iteration: 759; Percent complete: 75.9%; Average loss: 0.0784\n","Iteration: 760; Percent complete: 76.0%; Average loss: 0.0666\n","Iteration: 761; Percent complete: 76.1%; Average loss: 0.0733\n","Iteration: 762; Percent complete: 76.2%; Average loss: 0.0609\n","Iteration: 763; Percent complete: 76.3%; Average loss: 0.0595\n","Iteration: 764; Percent complete: 76.4%; Average loss: 0.0846\n","Iteration: 765; Percent complete: 76.5%; Average loss: 0.0667\n","Iteration: 766; Percent complete: 76.6%; Average loss: 0.0611\n","Iteration: 767; Percent complete: 76.7%; Average loss: 0.0551\n","Iteration: 768; Percent complete: 76.8%; Average loss: 0.0692\n","Iteration: 769; Percent complete: 76.9%; Average loss: 0.0550\n","Iteration: 770; Percent complete: 77.0%; Average loss: 0.0641\n","Iteration: 771; Percent complete: 77.1%; Average loss: 0.0779\n","Iteration: 772; Percent complete: 77.2%; Average loss: 0.0638\n","Iteration: 773; Percent complete: 77.3%; Average loss: 0.0844\n","Iteration: 774; Percent complete: 77.4%; Average loss: 0.0738\n","Iteration: 775; Percent complete: 77.5%; Average loss: 0.0523\n","Iteration: 776; Percent complete: 77.6%; Average loss: 0.0591\n","Iteration: 777; Percent complete: 77.7%; Average loss: 0.0521\n","Iteration: 778; Percent complete: 77.8%; Average loss: 0.0671\n","Iteration: 779; Percent complete: 77.9%; Average loss: 0.0696\n","Iteration: 780; Percent complete: 78.0%; Average loss: 0.0605\n","Iteration: 781; Percent complete: 78.1%; Average loss: 0.0535\n","Iteration: 782; Percent complete: 78.2%; Average loss: 0.0555\n","Iteration: 783; Percent complete: 78.3%; Average loss: 0.0579\n","Iteration: 784; Percent complete: 78.4%; Average loss: 0.0622\n","Iteration: 785; Percent complete: 78.5%; Average loss: 0.0524\n","Iteration: 786; Percent complete: 78.6%; Average loss: 0.0620\n","Iteration: 787; Percent complete: 78.7%; Average loss: 0.0641\n","Iteration: 788; Percent complete: 78.8%; Average loss: 0.0464\n","Iteration: 789; Percent complete: 78.9%; Average loss: 0.0571\n","Iteration: 790; Percent complete: 79.0%; Average loss: 0.0557\n","Iteration: 791; Percent complete: 79.1%; Average loss: 0.0545\n","Iteration: 792; Percent complete: 79.2%; Average loss: 0.0498\n","Iteration: 793; Percent complete: 79.3%; Average loss: 0.0532\n","Iteration: 794; Percent complete: 79.4%; Average loss: 0.0660\n","Iteration: 795; Percent complete: 79.5%; Average loss: 0.0486\n","Iteration: 796; Percent complete: 79.6%; Average loss: 0.0527\n","Iteration: 797; Percent complete: 79.7%; Average loss: 0.0562\n","Iteration: 798; Percent complete: 79.8%; Average loss: 0.0608\n","Iteration: 799; Percent complete: 79.9%; Average loss: 0.0588\n","Iteration: 800; Percent complete: 80.0%; Average loss: 0.0608\n","Iteration: 801; Percent complete: 80.1%; Average loss: 0.0475\n","Iteration: 802; Percent complete: 80.2%; Average loss: 0.0634\n","Iteration: 803; Percent complete: 80.3%; Average loss: 0.0538\n","Iteration: 804; Percent complete: 80.4%; Average loss: 0.0534\n","Iteration: 805; Percent complete: 80.5%; Average loss: 0.0547\n","Iteration: 806; Percent complete: 80.6%; Average loss: 0.0547\n","Iteration: 807; Percent complete: 80.7%; Average loss: 0.0519\n","Iteration: 808; Percent complete: 80.8%; Average loss: 0.0458\n","Iteration: 809; Percent complete: 80.9%; Average loss: 0.0592\n","Iteration: 810; Percent complete: 81.0%; Average loss: 0.0538\n","Iteration: 811; Percent complete: 81.1%; Average loss: 0.0586\n","Iteration: 812; Percent complete: 81.2%; Average loss: 0.0567\n","Iteration: 813; Percent complete: 81.3%; Average loss: 0.0466\n","Iteration: 814; Percent complete: 81.4%; Average loss: 0.0498\n","Iteration: 815; Percent complete: 81.5%; Average loss: 0.0510\n","Iteration: 816; Percent complete: 81.6%; Average loss: 0.0447\n","Iteration: 817; Percent complete: 81.7%; Average loss: 0.0843\n","Iteration: 818; Percent complete: 81.8%; Average loss: 0.0563\n","Iteration: 819; Percent complete: 81.9%; Average loss: 0.0454\n","Iteration: 820; Percent complete: 82.0%; Average loss: 0.0545\n","Iteration: 821; Percent complete: 82.1%; Average loss: 0.0495\n","Iteration: 822; Percent complete: 82.2%; Average loss: 0.0576\n","Iteration: 823; Percent complete: 82.3%; Average loss: 0.0486\n","Iteration: 824; Percent complete: 82.4%; Average loss: 0.0504\n","Iteration: 825; Percent complete: 82.5%; Average loss: 0.0499\n","Iteration: 826; Percent complete: 82.6%; Average loss: 0.0492\n","Iteration: 827; Percent complete: 82.7%; Average loss: 0.0507\n","Iteration: 828; Percent complete: 82.8%; Average loss: 0.0517\n","Iteration: 829; Percent complete: 82.9%; Average loss: 0.0469\n","Iteration: 830; Percent complete: 83.0%; Average loss: 0.0547\n","Iteration: 831; Percent complete: 83.1%; Average loss: 0.0455\n","Iteration: 832; Percent complete: 83.2%; Average loss: 0.0457\n","Iteration: 833; Percent complete: 83.3%; Average loss: 0.0435\n","Iteration: 834; Percent complete: 83.4%; Average loss: 0.0419\n","Iteration: 835; Percent complete: 83.5%; Average loss: 0.0512\n","Iteration: 836; Percent complete: 83.6%; Average loss: 0.0536\n","Iteration: 837; Percent complete: 83.7%; Average loss: 0.0361\n","Iteration: 838; Percent complete: 83.8%; Average loss: 0.0488\n","Iteration: 839; Percent complete: 83.9%; Average loss: 0.0506\n","Iteration: 840; Percent complete: 84.0%; Average loss: 0.0377\n","Iteration: 841; Percent complete: 84.1%; Average loss: 0.0493\n","Iteration: 842; Percent complete: 84.2%; Average loss: 0.0462\n","Iteration: 843; Percent complete: 84.3%; Average loss: 0.0393\n","Iteration: 844; Percent complete: 84.4%; Average loss: 0.0441\n","Iteration: 845; Percent complete: 84.5%; Average loss: 0.0458\n","Iteration: 846; Percent complete: 84.6%; Average loss: 0.0378\n","Iteration: 847; Percent complete: 84.7%; Average loss: 0.0467\n","Iteration: 848; Percent complete: 84.8%; Average loss: 0.0406\n","Iteration: 849; Percent complete: 84.9%; Average loss: 0.0407\n","Iteration: 850; Percent complete: 85.0%; Average loss: 0.0425\n","Iteration: 851; Percent complete: 85.1%; Average loss: 0.0410\n","Iteration: 852; Percent complete: 85.2%; Average loss: 0.0418\n","Iteration: 853; Percent complete: 85.3%; Average loss: 0.0465\n","Iteration: 854; Percent complete: 85.4%; Average loss: 0.0466\n","Iteration: 855; Percent complete: 85.5%; Average loss: 0.0457\n","Iteration: 856; Percent complete: 85.6%; Average loss: 0.0387\n","Iteration: 857; Percent complete: 85.7%; Average loss: 0.0457\n","Iteration: 858; Percent complete: 85.8%; Average loss: 0.0462\n","Iteration: 859; Percent complete: 85.9%; Average loss: 0.0439\n","Iteration: 860; Percent complete: 86.0%; Average loss: 0.0415\n","Iteration: 861; Percent complete: 86.1%; Average loss: 0.0443\n","Iteration: 862; Percent complete: 86.2%; Average loss: 0.0409\n","Iteration: 863; Percent complete: 86.3%; Average loss: 0.0405\n","Iteration: 864; Percent complete: 86.4%; Average loss: 0.0508\n","Iteration: 865; Percent complete: 86.5%; Average loss: 0.0503\n","Iteration: 866; Percent complete: 86.6%; Average loss: 0.0390\n","Iteration: 867; Percent complete: 86.7%; Average loss: 0.0434\n","Iteration: 868; Percent complete: 86.8%; Average loss: 0.0444\n","Iteration: 869; Percent complete: 86.9%; Average loss: 0.0434\n","Iteration: 870; Percent complete: 87.0%; Average loss: 0.0316\n","Iteration: 871; Percent complete: 87.1%; Average loss: 0.0393\n","Iteration: 872; Percent complete: 87.2%; Average loss: 0.0485\n","Iteration: 873; Percent complete: 87.3%; Average loss: 0.0435\n","Iteration: 874; Percent complete: 87.4%; Average loss: 0.0426\n","Iteration: 875; Percent complete: 87.5%; Average loss: 0.0387\n","Iteration: 876; Percent complete: 87.6%; Average loss: 0.0393\n","Iteration: 877; Percent complete: 87.7%; Average loss: 0.0423\n","Iteration: 878; Percent complete: 87.8%; Average loss: 0.0320\n","Iteration: 879; Percent complete: 87.9%; Average loss: 0.0324\n","Iteration: 880; Percent complete: 88.0%; Average loss: 0.0380\n","Iteration: 881; Percent complete: 88.1%; Average loss: 0.0364\n","Iteration: 882; Percent complete: 88.2%; Average loss: 0.0444\n","Iteration: 883; Percent complete: 88.3%; Average loss: 0.0373\n","Iteration: 884; Percent complete: 88.4%; Average loss: 0.0360\n","Iteration: 885; Percent complete: 88.5%; Average loss: 0.0454\n","Iteration: 886; Percent complete: 88.6%; Average loss: 0.0306\n","Iteration: 887; Percent complete: 88.7%; Average loss: 0.0440\n","Iteration: 888; Percent complete: 88.8%; Average loss: 0.0390\n","Iteration: 889; Percent complete: 88.9%; Average loss: 0.0351\n","Iteration: 890; Percent complete: 89.0%; Average loss: 0.0382\n","Iteration: 891; Percent complete: 89.1%; Average loss: 0.0318\n","Iteration: 892; Percent complete: 89.2%; Average loss: 0.0381\n","Iteration: 893; Percent complete: 89.3%; Average loss: 0.0302\n","Iteration: 894; Percent complete: 89.4%; Average loss: 0.0415\n","Iteration: 895; Percent complete: 89.5%; Average loss: 0.0342\n","Iteration: 896; Percent complete: 89.6%; Average loss: 0.0321\n","Iteration: 897; Percent complete: 89.7%; Average loss: 0.0356\n","Iteration: 898; Percent complete: 89.8%; Average loss: 0.0389\n","Iteration: 899; Percent complete: 89.9%; Average loss: 0.0367\n","Iteration: 900; Percent complete: 90.0%; Average loss: 0.0285\n","Iteration: 901; Percent complete: 90.1%; Average loss: 0.0325\n","Iteration: 902; Percent complete: 90.2%; Average loss: 0.0313\n","Iteration: 903; Percent complete: 90.3%; Average loss: 0.0328\n","Iteration: 904; Percent complete: 90.4%; Average loss: 0.0376\n","Iteration: 905; Percent complete: 90.5%; Average loss: 0.0322\n","Iteration: 906; Percent complete: 90.6%; Average loss: 0.0339\n","Iteration: 907; Percent complete: 90.7%; Average loss: 0.0308\n","Iteration: 908; Percent complete: 90.8%; Average loss: 0.0295\n","Iteration: 909; Percent complete: 90.9%; Average loss: 0.0351\n","Iteration: 910; Percent complete: 91.0%; Average loss: 0.0308\n","Iteration: 911; Percent complete: 91.1%; Average loss: 0.0325\n","Iteration: 912; Percent complete: 91.2%; Average loss: 0.0326\n","Iteration: 913; Percent complete: 91.3%; Average loss: 0.0324\n","Iteration: 914; Percent complete: 91.4%; Average loss: 0.0338\n","Iteration: 915; Percent complete: 91.5%; Average loss: 0.0294\n","Iteration: 916; Percent complete: 91.6%; Average loss: 0.0277\n","Iteration: 917; Percent complete: 91.7%; Average loss: 0.0302\n","Iteration: 918; Percent complete: 91.8%; Average loss: 0.0310\n","Iteration: 919; Percent complete: 91.9%; Average loss: 0.0301\n","Iteration: 920; Percent complete: 92.0%; Average loss: 0.0307\n","Iteration: 921; Percent complete: 92.1%; Average loss: 0.0392\n","Iteration: 922; Percent complete: 92.2%; Average loss: 0.0305\n","Iteration: 923; Percent complete: 92.3%; Average loss: 0.0309\n","Iteration: 924; Percent complete: 92.4%; Average loss: 0.0430\n","Iteration: 925; Percent complete: 92.5%; Average loss: 0.0363\n","Iteration: 926; Percent complete: 92.6%; Average loss: 0.0351\n","Iteration: 927; Percent complete: 92.7%; Average loss: 0.0324\n","Iteration: 928; Percent complete: 92.8%; Average loss: 0.0293\n","Iteration: 929; Percent complete: 92.9%; Average loss: 0.0271\n","Iteration: 930; Percent complete: 93.0%; Average loss: 0.0269\n","Iteration: 931; Percent complete: 93.1%; Average loss: 0.0291\n","Iteration: 932; Percent complete: 93.2%; Average loss: 0.0296\n","Iteration: 933; Percent complete: 93.3%; Average loss: 0.0329\n","Iteration: 934; Percent complete: 93.4%; Average loss: 0.0372\n","Iteration: 935; Percent complete: 93.5%; Average loss: 0.0277\n","Iteration: 936; Percent complete: 93.6%; Average loss: 0.0360\n","Iteration: 937; Percent complete: 93.7%; Average loss: 0.0396\n","Iteration: 938; Percent complete: 93.8%; Average loss: 0.0330\n","Iteration: 939; Percent complete: 93.9%; Average loss: 0.0412\n","Iteration: 940; Percent complete: 94.0%; Average loss: 0.0277\n","Iteration: 941; Percent complete: 94.1%; Average loss: 0.0286\n","Iteration: 942; Percent complete: 94.2%; Average loss: 0.0255\n","Iteration: 943; Percent complete: 94.3%; Average loss: 0.0296\n","Iteration: 944; Percent complete: 94.4%; Average loss: 0.0341\n","Iteration: 945; Percent complete: 94.5%; Average loss: 0.0282\n","Iteration: 946; Percent complete: 94.6%; Average loss: 0.0266\n","Iteration: 947; Percent complete: 94.7%; Average loss: 0.0304\n","Iteration: 948; Percent complete: 94.8%; Average loss: 0.0283\n","Iteration: 949; Percent complete: 94.9%; Average loss: 0.0278\n","Iteration: 950; Percent complete: 95.0%; Average loss: 0.0328\n","Iteration: 951; Percent complete: 95.1%; Average loss: 0.0242\n","Iteration: 952; Percent complete: 95.2%; Average loss: 0.0268\n","Iteration: 953; Percent complete: 95.3%; Average loss: 0.0294\n","Iteration: 954; Percent complete: 95.4%; Average loss: 0.0273\n","Iteration: 955; Percent complete: 95.5%; Average loss: 0.0287\n","Iteration: 956; Percent complete: 95.6%; Average loss: 0.0300\n","Iteration: 957; Percent complete: 95.7%; Average loss: 0.0303\n","Iteration: 958; Percent complete: 95.8%; Average loss: 0.0350\n","Iteration: 959; Percent complete: 95.9%; Average loss: 0.0244\n","Iteration: 960; Percent complete: 96.0%; Average loss: 0.0303\n","Iteration: 961; Percent complete: 96.1%; Average loss: 0.0269\n","Iteration: 962; Percent complete: 96.2%; Average loss: 0.0382\n","Iteration: 963; Percent complete: 96.3%; Average loss: 0.0249\n","Iteration: 964; Percent complete: 96.4%; Average loss: 0.0272\n","Iteration: 965; Percent complete: 96.5%; Average loss: 0.0240\n","Iteration: 966; Percent complete: 96.6%; Average loss: 0.0285\n","Iteration: 967; Percent complete: 96.7%; Average loss: 0.0313\n","Iteration: 968; Percent complete: 96.8%; Average loss: 0.0262\n","Iteration: 969; Percent complete: 96.9%; Average loss: 0.0263\n","Iteration: 970; Percent complete: 97.0%; Average loss: 0.0248\n","Iteration: 971; Percent complete: 97.1%; Average loss: 0.0278\n","Iteration: 972; Percent complete: 97.2%; Average loss: 0.0260\n","Iteration: 973; Percent complete: 97.3%; Average loss: 0.0257\n","Iteration: 974; Percent complete: 97.4%; Average loss: 0.0264\n","Iteration: 975; Percent complete: 97.5%; Average loss: 0.0370\n","Iteration: 976; Percent complete: 97.6%; Average loss: 0.0242\n","Iteration: 977; Percent complete: 97.7%; Average loss: 0.0303\n","Iteration: 978; Percent complete: 97.8%; Average loss: 0.0253\n","Iteration: 979; Percent complete: 97.9%; Average loss: 0.0269\n","Iteration: 980; Percent complete: 98.0%; Average loss: 0.0264\n","Iteration: 981; Percent complete: 98.1%; Average loss: 0.0260\n","Iteration: 982; Percent complete: 98.2%; Average loss: 0.0253\n","Iteration: 983; Percent complete: 98.3%; Average loss: 0.0282\n","Iteration: 984; Percent complete: 98.4%; Average loss: 0.0253\n","Iteration: 985; Percent complete: 98.5%; Average loss: 0.0243\n","Iteration: 986; Percent complete: 98.6%; Average loss: 0.0385\n","Iteration: 987; Percent complete: 98.7%; Average loss: 0.0370\n","Iteration: 988; Percent complete: 98.8%; Average loss: 0.0231\n","Iteration: 989; Percent complete: 98.9%; Average loss: 0.0239\n","Iteration: 990; Percent complete: 99.0%; Average loss: 0.0244\n","Iteration: 991; Percent complete: 99.1%; Average loss: 0.0222\n","Iteration: 992; Percent complete: 99.2%; Average loss: 0.0256\n","Iteration: 993; Percent complete: 99.3%; Average loss: 0.0246\n","Iteration: 994; Percent complete: 99.4%; Average loss: 0.0240\n","Iteration: 995; Percent complete: 99.5%; Average loss: 0.0304\n","Iteration: 996; Percent complete: 99.6%; Average loss: 0.0228\n","Iteration: 997; Percent complete: 99.7%; Average loss: 0.0243\n","Iteration: 998; Percent complete: 99.8%; Average loss: 0.0235\n","Iteration: 999; Percent complete: 99.9%; Average loss: 0.0266\n","Iteration: 1000; Percent complete: 100.0%; Average loss: 0.0262\n"]}]},{"cell_type":"markdown","metadata":{"id":"uukZotTVRHeu"},"source":["### Run Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMupGMTsRKY2","executionInfo":{"status":"ok","timestamp":1637655181792,"user_tz":-480,"elapsed":27356,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"18a03a7d-2ec1-40de-c8d5-6a6cc0b332a6"},"source":["# Set dropout layers to eval mode\n","encoder.eval()\n","decoder.eval()\n","\n","# Initialize search module\n","searcher = GreedySearchDecoder(encoder, decoder)\n","\n","# Begin chatting (uncomment and run the following line to begin)\n","evaluateInput(encoder, decoder, searcher, voc)"],"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":["> Hi\n","Homer Bot: hi .\n","> What's up?\n","Homer Bot: oh !\n","> How are you?\n","Homer Bot: no !\n","> Goodbye\n","Homer Bot: good bye ? where s my clean underwear ?\n","> q\n"]}]},{"cell_type":"markdown","metadata":{"id":"_lBm_gE4cJfO"},"source":["### Evaluate against test queries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t47hwlaLcPcM","executionInfo":{"status":"ok","timestamp":1637655208124,"user_tz":-480,"elapsed":23198,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"82d276eb-ddd6-4777-a050-43d390e9df9e"},"source":["!pip install tabulate"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u97rPihfZ0Hi","executionInfo":{"status":"ok","timestamp":1637655208921,"user_tz":-480,"elapsed":800,"user":{"displayName":"LAM SONG KAI, SEAN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09705056550756356988"}},"outputId":"c1b49f37-5c35-42de-e015-d454d96c0e21"},"source":["from tabulate import tabulate\n","\n","results = []\n","\n","for q,a in test_pairs:\n","  try:\n","    # Normalize sentence\n","    input_sentence = normalizeString(q)\n","    # Evaluate sentence\n","    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n","    # Format and print response sentence\n","    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","    output = ' '.join(output_words)\n","  except KeyError:\n","    output = \"I don't understand.\" \n","\n","  results.append([q,output,normalizeString(a)])\n","\n","print(tabulate(results,headers=[\"Query\",\"Response\",\"Original response\"]))\n"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Query                                  Response                                  Original response\n","-------------------------------------  ----------------------------------------  -------------------------------------\n","he s gone . forever .                  i mean that .                             and ?\n","no !                                   pork chops ?                              nuts .\n","that was our wedding .                 i think you know a father .               oh .\n","you re so silly .                      i love you boy .                          gimme a banana .\n","no .                                   marco !                                   would you have to do extra work ?\n","uh gee homer don t quit . . .          what ?                                    all right then .\n","come here !                            you re gonna be sorry you ll take that .  where where are we going ?\n","they do .                              say he . . .                              oh .\n","get back in bed .                      it s all about trust honey .              no .\n","oh yeah sure .                         woo hoo !                                 watch .\n","really ?                               thank you .                               nah .\n","you know homer i am your supervisor .  i can t even tell you my name .           sorry sir .\n","homer . . . breakfast is ready .       what ?                                    hmmm . . .\n","the last place i saw it . . .          oh right .                                uh huh .\n","that s right .                         i see your point brother .                well i m still gonna run !\n","wait dad he s smiling .                what ?                                    mr . burns you brought us a present .\n","it doesn t do anything .               yes it is .                               marge ! really what does it do ?\n","of course .                            i mean four !                             not insane .\n","huh ?                                  no .                                      huh ? huh ?\n","important .                            goodnight .                               ooh . . . how about the v ?\n"]}]},{"cell_type":"markdown","metadata":{"id":"us_lEOoXRMoP"},"source":["## Save Checkpoint"]},{"cell_type":"code","metadata":{"id":"cmFMdIp0ROYr"},"source":["#zip and save checkpoint\n","!zip -r \"/content/data.zip\" \"/content/data\""],"execution_count":null,"outputs":[]}]}